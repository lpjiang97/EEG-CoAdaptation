{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Error Decoder\n",
    "Use this clean notebook to create the decoder that will classify Error-related Potentials (ErrPs). Please see `Exploration_ErrP_Template_Detection.ipnyb` for the full exploration.\n",
    "\n",
    "Nile Wilson 2019.01.23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.fftpack import fft, ifft\n",
    "from scipy import signal\n",
    "from mne.filter import filter_data\n",
    "\n",
    "import scipy.signal as scisig\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import pickle\n",
    "import csv\n",
    "import mne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadEEGData(filename, EEGdevice):\n",
    "    \"\"\" This function converts a single .easy file (from NIC2) to an easy-to-use dataframe.\n",
    "    Uses both the .easy file and .info file (containing metadata)\n",
    "    \n",
    "    ---- Input ----\n",
    "    filename: string containing the .easy filepath\n",
    "    \n",
    "    ---- Output ----\n",
    "    df: dataframe containing all the EEG, accelerometer, and event marker data\n",
    "    fs: sampling rate for the EEG data (Hz)\n",
    "    fs_accel: sampling rate for the accelerometer data (Hz)\n",
    "    \n",
    "    \"\"\"\n",
    "    if EEGdevice == 7:\n",
    "        x = 1\n",
    "    elif EEGdevice == 8:\n",
    "        # Read in the .easy file\n",
    "        df = pd.read_csv(filename, delimiter='\\t', header=None)\n",
    "\n",
    "        # Get metadata from the .info file\n",
    "        fname = filename[:-5] + '.info'\n",
    "        with open(fname) as f:\n",
    "            content = f.readlines()\n",
    "        content = [x.strip() for x in content]\n",
    "\n",
    "        # Get the channel names\n",
    "        channel_info = [x for x in content if 'Channel ' in x]\n",
    "        channel_names = []\n",
    "        for ch in range(len(channel_info)):\n",
    "            channel_names.append(channel_info[ch].split(': ')[1])\n",
    "\n",
    "        channel_names.append('X')\n",
    "        channel_names.append('Y')\n",
    "        channel_names.append('Z')\n",
    "        channel_names.append('STI 014')\n",
    "        channel_names.append('DateTime')\n",
    "\n",
    "        # Get sampling rates\n",
    "        sampling_rates = [x for x in content if 'sampling rate: ' in x]\n",
    "        fs_all = []\n",
    "        for freq in range(len(sampling_rates)):\n",
    "            tmp = sampling_rates[freq].split(': ')[1].split(' ')[0]\n",
    "            if tmp in ['N/A']:\n",
    "                print('Skipping N/A')\n",
    "            else:\n",
    "                fs_all.append(float(sampling_rates[freq].split(': ')[1].split(' ')[0]))\n",
    "\n",
    "        # Store sampling rates\n",
    "        fs = fs_all[0]\n",
    "        fs_accel = fs_all[1]\n",
    "\n",
    "        # Assign the column names\n",
    "        df.columns = channel_names\n",
    "    \n",
    "    # Return dataframe and sampling rates\n",
    "    return df, fs, fs_accel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadBehavioralData(filename_behavioral):\n",
    "    \"\"\"\n",
    "    This function loads behavioral data for the motor screening task and formats it to use in this script\n",
    "    \"\"\"\n",
    "    behavioralData = pd.read_csv(filename_behavioral, ',')\n",
    "    behavioralHeader = behavioralData.columns\n",
    "    behavioralData.head()\n",
    "    \n",
    "    return behavioralData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SyncTriggerPulses(EEGdata, EEGdevice, fs, behavioralData):\n",
    "    \"\"\"\n",
    "    This function returns the indices for events of interest\n",
    "    \"\"\"\n",
    "    \n",
    "    if EEGdevice == 7:\n",
    "        print('Put code here')\n",
    "    elif EEGdevice == 8:\n",
    "        \n",
    "        # Ignore the first error if the first movement was error\n",
    "        # because the pulse value would be 0 and we wouldn't able to log that in index_trigger\n",
    "        behavioral_error_trials = np.where(behavioralData['error_induced']==True)[0]\n",
    "\n",
    "        # Only store info for trials that are confirmed to be error movement from both the EEG data and from the behavioral data\n",
    "        # Start from 1 and not 0 because 0 is used for no pulse sent as well\n",
    "        error_trials = np.intersect1d(np.unique(EEGdata['STI 014'])[1:], behavioral_error_trials)\n",
    "        \n",
    "        index_trigger = list()\n",
    "        for i in range(0,len(error_trials)):\n",
    "            index_trigger.append(np.where(EEGdata['STI 014'] == error_trials[i])[0][-1])\n",
    "        \n",
    "        # Check number of trials\n",
    "        num_of_trials = behavioralData.shape[0]\n",
    "        num_of_error = len(index_trigger)\n",
    "        num_of_correct = num_of_trials - num_of_error - 1 # skipped the initial error movement because it had pulseValue 0\n",
    "        \n",
    "        # Get the movement start times\n",
    "        t_diff_all = list()\n",
    "        for i in range(0, len(index_trigger)):\n",
    "            t_diff_all.append((index_trigger[i]/fs)-behavioralData['time'][error_trials[i]])\n",
    "        \n",
    "        # Add t_0 to find the times in the EEG data\n",
    "        t_0 = np.mean(t_diff_all)\n",
    "        t_trial_start = list()\n",
    "        for i in range(0, num_of_trials):\n",
    "            t_trial_start.append(behavioralData['time'][i] + t_0)\n",
    "            \n",
    "        # Get a nice list of whether a trial is error or not\n",
    "        isError = list()\n",
    "        error_trials = list()\n",
    "        correct_trials = list()\n",
    "        for i in range(0, len(behavioralData['error_induced'])):\n",
    "            if behavioralData['error_induced'][i] == True:\n",
    "                isError.append(1)\n",
    "                error_trials.append(i)\n",
    "            else:\n",
    "                isError.append(0)\n",
    "                correct_trials.append(i)\n",
    "\n",
    "        correct_trials = np.asarray(correct_trials)\n",
    "\n",
    "    \n",
    "    return num_of_trials, t_trial_start, error_trials, correct_trials, isError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EpochData(EEGdata, t_trial_start):\n",
    "    \"\"\"\n",
    "    This function epochs the data\n",
    "    \"\"\"\n",
    "    \n",
    "    if EEGdevice == 7:\n",
    "        channels = EEGdata.columns[1:8]\n",
    "    elif EEGdevice == 8:\n",
    "        channels = EEGdata.columns[0:8]\n",
    "    \n",
    "    epochs_norm = []\n",
    "\n",
    "    for trial in range(0,len(t_trial_start)):\n",
    "        t_start = np.round((t_trial_start[trial] - 0 ) * fs)\n",
    "        t_end = np.round((t_trial_start[trial] + 0.600) * fs)\n",
    "\n",
    "        # Baseline\n",
    "        tb_start = np.round((t_trial_start[trial] - 0.700 ) * fs)\n",
    "        tb_end = np.round((t_trial_start[trial] - 0.100) * fs)\n",
    "        baseline = EEGdata.loc[tb_start:tb_end][channels]\n",
    "\n",
    "        # Store epoch\n",
    "        tmp = (EEGdata.loc[t_start:t_end][channels] - np.mean(baseline))/np.std(baseline)\n",
    "        epochs_norm.append(tmp)\n",
    "    \n",
    "    return epochs_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateTemplates(epochs_norm, error_trials, correct_trials):\n",
    "    \"\"\"\n",
    "    Generate grand average waveform templates for both error and correct responses for all channels\n",
    "    \"\"\"\n",
    "    \n",
    "    if EEGdevice == 7:\n",
    "        channels = EEGdata.columns[1:8]\n",
    "    elif EEGdevice == 8:\n",
    "        channels = EEGdata.columns[0:8]\n",
    "        \n",
    "    # Get average timeseries waveform for error and correct\n",
    "    channelsOfInt = channels\n",
    "    error_template = dict()\n",
    "    correct_template = dict()\n",
    "\n",
    "    for chanOfInt in channelsOfInt:\n",
    "        tmp_to_avg_error = list()\n",
    "        tmp_to_avg_correct = list()\n",
    "\n",
    "        for trial in error_trials:\n",
    "            tmp = epochs_norm[trial][chanOfInt].values\n",
    "            tmp_to_avg_error.append(tmp)\n",
    "\n",
    "        for trial in correct_trials:\n",
    "            tmp = epochs_norm[trial][chanOfInt].values\n",
    "            tmp_to_avg_correct.append(tmp)\n",
    "\n",
    "        # Plot the average in bold black\n",
    "        error_template[chanOfInt] = np.mean(tmp_to_avg_error,0)\n",
    "        correct_template[chanOfInt] = np.mean(tmp_to_avg_correct,0)\n",
    "    \n",
    "    return error_template, correct_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractFeatures(epochs, num_of_trials, error_template, correct_template, featureType):\n",
    "    \"\"\"\n",
    "    Extract signal features of interest\n",
    "    featureType:    'template' or 'frequency'. 'template' returns features based on the template projection values\n",
    "                    for individual epochs with the error and correct templates. 'frequency' returns features that\n",
    "                    are just delta and theta power for each channel for the epochs\n",
    "    \"\"\"\n",
    "    \n",
    "    if featureType in ['template','Template','TEMPLATE','t','T','projection','Projection','PROJECTION','p','P']:\n",
    "        # template_projection[chanOfInt] will have two columns\n",
    "        # col 1: how well the trial signal matches with the error signal template\n",
    "        # col 2: how well the trial signal matches with the correct signal template\n",
    "        projections_all = dict()\n",
    "        channelsToUse = error_template.keys()\n",
    "\n",
    "        for chanOfInt in channelsToUse:\n",
    "            projections = np.zeros([2, num_of_trials])\n",
    "            for trial in range(0, num_of_trials):\n",
    "                # Individual epoch (normalized)\n",
    "                tmp = epochs_norm[trial][chanOfInt]\n",
    "                a = tmp\n",
    "\n",
    "                # Template waveform for error (normalized)\n",
    "                tmp0 = error_template[chanOfInt]\n",
    "                tmp_norm = (tmp0 - np.mean(tmp0))/np.std(tmp0)\n",
    "                b = tmp_norm\n",
    "\n",
    "                # Template waveform for correct (normalized)\n",
    "                tmp = correct_template[chanOfInt]\n",
    "                tmp_norm = (tmp - np.mean(tmp0))/np.std(tmp0)\n",
    "                c = tmp_norm\n",
    "\n",
    "                # Store sum of convolutions\n",
    "\n",
    "                projections[0][trial] = np.sum(np.convolve(a,b,'same'))\n",
    "                projections[1][trial] = np.sum(np.convolve(a,c,'same'))\n",
    "\n",
    "            projections_all[chanOfInt] = projections\n",
    "        \n",
    "        # Organize the features\n",
    "        channels = list(projections_all.keys())\n",
    "        num_of_features = np.shape(projections_all['Cz'])[0] * len(channels)\n",
    "        channels_full = list(projections_all.keys()) * 2\n",
    "        num_of_trials = np.shape(projections_all['Cz'])[1]\n",
    "\n",
    "        features = np.zeros([num_of_features, num_of_trials])\n",
    "\n",
    "        for trial in range(0, num_of_trials):\n",
    "            # Error trials are 0 to num_of_features//2, and correct trials are num_of_features//2 to num_of_features\n",
    "            for feature in range(0, num_of_features):\n",
    "                features[feature, trial] = projections_all[channels_full[feature]][0][trial]\n",
    "            \n",
    "    elif featureType in ['frequency','Frequency','FREQUENCY','f','F']:\n",
    "        channelsToUse = error_template.keys()\n",
    "        delta_power = dict.fromkeys(channelsToUse)\n",
    "        theta_power = dict.fromkeys(channelsToUse)\n",
    "        ds_f = 1 # downsampling factor\n",
    "\n",
    "        for chanOfInt in channelsToUse:\n",
    "            tmp_delta = list()\n",
    "            tmp_theta = list()\n",
    "\n",
    "            for trial in range(0, num_of_trials):\n",
    "                f, Pxx_den = signal.welch(signal.decimate(epochs_norm[trial][chanOfInt],ds_f), fs/ds_f, scaling='spectrum')\n",
    "                delta_idx = np.where(np.round(f) <= 4)\n",
    "                tmp_delta.append(np.sum(Pxx_den[delta_idx]))\n",
    "\n",
    "                theta_idx = np.where(np.logical_and(np.round(f) > 4, np.round(f) <= 7))\n",
    "                tmp_theta.append(np.sum(Pxx_den[theta_idx]))\n",
    "\n",
    "            delta_power[chanOfInt] = tmp_delta\n",
    "            theta_power[chanOfInt] = tmp_theta\n",
    "            \n",
    "        # Organize the features\n",
    "        num_of_examples = len(delta_power['Cz'])\n",
    "        num_of_features = len(delta_power.keys()) + len(theta_power.keys()) \n",
    "        features = np.zeros([num_of_features, num_of_examples])\n",
    "\n",
    "        # Get all channels in one list to loop through\n",
    "        feature_channels = np.concatenate([np.asarray(list(delta_power.keys())),np.asarray(list(theta_power.keys()))])\n",
    "\n",
    "        for i in range(0, num_of_examples):\n",
    "            for j in range(0, num_of_features//2):\n",
    "                features[j, i] = delta_power[feature_channels[j]][i]\n",
    "            for j in range(num_of_features//2, num_of_features):\n",
    "                features[j, i] = theta_power[feature_channels[j]][i]\n",
    "\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainDecoder(features, isError):\n",
    "    \"\"\"\n",
    "    Trains the decoder on ALL the data (does not split into test and train because this is all train)\n",
    "    \"\"\"\n",
    "    # Fit a simple neural network (multilayer perceptron)\n",
    "    X = features.T\n",
    "    y = isError\n",
    "\n",
    "    # Preprocess dataset, split into training and test part\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # Resample to account for imbalance\n",
    "    method = SMOTE(kind='regular')\n",
    "    X, y = method.fit_sample(X, y)\n",
    "\n",
    "    # Determine model parameters\n",
    "    activations = ['relu','tanh']\n",
    "    alphas = np.logspace(-6, 3, 10)\n",
    "    solvers = ['lbfgs','sgd']\n",
    "    hyper_params = {\"activation\":activations, \"alpha\":alphas, \"solver\":solvers}\n",
    "    grid = GridSearchCV(MLPClassifier(learning_rate='constant', random_state=1), param_grid=hyper_params, cv=KFold(n_splits=5), verbose=True)\n",
    "    \n",
    "    # Fit the model\n",
    "    grid.fit(X, y)\n",
    "    clf = grid.best_estimator_\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    \"\"\"\n",
    "    # Fit the model\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3)\n",
    "    grid.fit(X_train, y_train)\n",
    "    clf = grid.best_estimator_\n",
    "    clf.fit(X_train, y_train)\n",
    "    score = clf.score(X_test, y_test)\n",
    "\n",
    "    print(grid.best_estimator_)\n",
    "    print('-----------')\n",
    "    print('score: ' + str(score))\n",
    "    print(confusion_matrix(y_test, clf.predict(X_test)))\n",
    "    print('-----------')\n",
    "    \"\"\"\n",
    "    \n",
    "    return clf, X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SaveDecoderAndData(clf, X, y, subjID, featureType, error_template, correct_template):\n",
    "    \"\"\"\n",
    "    Save the decoder and the data it was trained/tested on\n",
    "    \"\"\"\n",
    "    time_to_save = datetime.datetime.now().isoformat()\n",
    "    time_to_save = time_to_save.replace('T','-')\n",
    "    time_to_save = time_to_save.replace(':','-')\n",
    "    \n",
    "    model = clf\n",
    "    model_file = subjID + '_Error_classifier_' + time_to_save[:19] + '.sav'\n",
    "    pickle.dump(model, open(model_file, 'wb'))\n",
    "    \n",
    "    filepath_export_data = subjID + '_data_for_Error_classifier_' + time_to_save[:19] + '.npz'\n",
    "    np.savez_compressed(filepath_export_data, X=X, y=y, featureType=featureType, error_template=error_template, correct_template=correct_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables to Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjID = 'nile'\n",
    "EEGdevice = 8 # 7 for DSI-7, 8 for Enobio\n",
    "filename_eeg = '../data/nile/20190130120351_nile_Error_Screening.easy'\n",
    "filename_behavioral = '../data/nile/Error_Screening_nile_R1.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping N/A\n",
      "Creating RawArray with float64 data, n_channels=7, n_times=348265\n",
      "    Range : 0 ... 348264 =      0.000 ...   696.528 secs\n",
      "Ready.\n",
      "Setting up band-pass filter from 1 - 40 Hz\n",
      "l_trans_bandwidth chosen to be 1.0 Hz\n",
      "h_trans_bandwidth chosen to be 10.0 Hz\n",
      "Filter length of 1651 samples (3.302 sec) selected\n"
     ]
    }
   ],
   "source": [
    "# Load EEG data\n",
    "EEGdata, fs, fs_accel = LoadEEGData(filename_eeg, EEGdevice)\n",
    "\n",
    "# Load behavioral data\n",
    "behavioralData = LoadBehavioralData(filename_behavioral)\n",
    "\n",
    "# Sync up trigger pulses\n",
    "num_of_trials, t_trial_start, error_trials, correct_trials, isError = SyncTriggerPulses(EEGdata, EEGdevice, fs, behavioralData)\n",
    "\n",
    "# Clean the data\n",
    "EEGdata_orig = EEGdata.copy()\n",
    "lf = 1\n",
    "hf = 40\n",
    "\n",
    "if EEGdevice == 7:\n",
    "    channels = EEGdata.columns[1:8]\n",
    "elif EEGdevice == 8:\n",
    "    channels = EEGdata.columns[0:8]\n",
    "\n",
    "# Format our data into an mne-friendly format\n",
    "ch_types = ['eeg']*len(channels)\n",
    "info = mne.create_info(ch_names=list(channels), sfreq=fs, ch_types=ch_types)\n",
    "rawData = EEGdata[channels].values\n",
    "rawData = np.transpose(rawData)\n",
    "raw = mne.io.array.RawArray(rawData, info)\n",
    "raw.set_montage(mne.channels.read_montage(kind='standard_1020'))\n",
    "raw.filter(l_freq=lf, h_freq=hf)\n",
    "\n",
    "# Make a copy of the original data just in case\n",
    "EEGdata[channels] = raw.get_data().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch data, get features, train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nilew\\AppData\\Local\\Continuum\\anaconda2\\envs\\py36\\lib\\site-packages\\scipy\\signal\\_arraytools.py:45: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  b = a[a_slice]\n",
      "C:\\Users\\nilew\\AppData\\Local\\Continuum\\anaconda2\\envs\\py36\\lib\\site-packages\\scipy\\signal\\signaltools.py:3463: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return y[sl]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\Users\\nilew\\AppData\\Local\\Continuum\\anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\nilew\\AppData\\Local\\Continuum\\anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\nilew\\AppData\\Local\\Continuum\\anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\nilew\\AppData\\Local\\Continuum\\anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\nilew\\AppData\\Local\\Continuum\\anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\nilew\\AppData\\Local\\Continuum\\anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\nilew\\AppData\\Local\\Continuum\\anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\nilew\\AppData\\Local\\Continuum\\anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\nilew\\AppData\\Local\\Continuum\\anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\nilew\\AppData\\Local\\Continuum\\anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\nilew\\AppData\\Local\\Continuum\\anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\nilew\\AppData\\Local\\Continuum\\anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\nilew\\AppData\\Local\\Continuum\\anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\nilew\\AppData\\Local\\Continuum\\anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\nilew\\AppData\\Local\\Continuum\\anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\nilew\\AppData\\Local\\Continuum\\anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\nilew\\AppData\\Local\\Continuum\\anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\nilew\\AppData\\Local\\Continuum\\anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\nilew\\AppData\\Local\\Continuum\\anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\nilew\\AppData\\Local\\Continuum\\anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:  1.1min finished\n"
     ]
    }
   ],
   "source": [
    "# Epoch the data\n",
    "epochs_norm = EpochData(EEGdata, t_trial_start)\n",
    "\n",
    "# Generate error and correct waveform templates\n",
    "error_template, correct_template = GenerateTemplates(epochs_norm, error_trials, correct_trials)\n",
    "\n",
    "# Get features\n",
    "featureType = 'frequency'\n",
    "features = ExtractFeatures(epochs_norm, num_of_trials, error_template, correct_template, featureType)\n",
    "\n",
    "# Train model\n",
    "clf, X, y = TrainDecoder(features, isError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save decoder and data it was trained/tested on\n",
    "SaveDecoderAndData(clf, X, y, subjID, featureType, error_template, correct_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
