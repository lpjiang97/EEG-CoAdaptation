{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check BCI ErrPs\n",
    "Use this clean notebook to check the Error-related Potentials (ErrPs) in the BCI data!\n",
    "\n",
    "Nile Wilson 2019.01.29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.fftpack import fft, ifft\n",
    "from scipy import signal\n",
    "from mne.filter import filter_data\n",
    "\n",
    "import scipy.signal as scisig\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import pickle\n",
    "import glob\n",
    "import csv\n",
    "import mne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for working with the BCI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadEEGData(filename, EEGdevice):\n",
    "    \"\"\" This function converts a single .easy file (from NIC2) to an easy-to-use dataframe.\n",
    "    Uses both the .easy file and .info file (containing metadata)\n",
    "    \n",
    "    ---- Input ----\n",
    "    filename: string containing the .easy filepath\n",
    "    \n",
    "    ---- Output ----\n",
    "    df: dataframe containing all the EEG, accelerometer, and event marker data\n",
    "    fs: sampling rate for the EEG data (Hz)\n",
    "    fs_accel: sampling rate for the accelerometer data (Hz)\n",
    "    \n",
    "    \"\"\"\n",
    "    if EEGdevice == 7:\n",
    "        x = 1\n",
    "    elif EEGdevice == 8:\n",
    "        # Read in the .easy file\n",
    "        df = pd.read_csv(filename, delimiter='\\t', header=None)\n",
    "\n",
    "        # Get metadata from the .info file\n",
    "        fname = filename[:-5] + '.info'\n",
    "        with open(fname) as f:\n",
    "            content = f.readlines()\n",
    "        content = [x.strip() for x in content]\n",
    "\n",
    "        # Get the channel names\n",
    "        channel_info = [x for x in content if 'Channel ' in x]\n",
    "        channel_names = []\n",
    "        for ch in range(len(channel_info)):\n",
    "            channel_names.append(channel_info[ch].split(': ')[1])\n",
    "\n",
    "        channel_names.append('X')\n",
    "        channel_names.append('Y')\n",
    "        channel_names.append('Z')\n",
    "        channel_names.append('STI 014')\n",
    "        channel_names.append('DateTime')\n",
    "\n",
    "        # Get sampling rates\n",
    "        sampling_rates = [x for x in content if 'sampling rate: ' in x]\n",
    "        fs_all = []\n",
    "        for freq in range(len(sampling_rates)):\n",
    "            tmp = sampling_rates[freq].split(': ')[1].split(' ')[0]\n",
    "            if tmp in ['N/A']:\n",
    "                print('Skipping N/A')\n",
    "            else:\n",
    "                fs_all.append(float(sampling_rates[freq].split(': ')[1].split(' ')[0]))\n",
    "\n",
    "        # Store sampling rates\n",
    "        fs = fs_all[0]\n",
    "        fs_accel = fs_all[1]\n",
    "\n",
    "        # Assign the column names\n",
    "        df.columns = channel_names\n",
    "    \n",
    "    # Return dataframe and sampling rates\n",
    "    return df, fs, fs_accel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadBehavioralDataBCI(filename_behavioral):\n",
    "    \"\"\"\n",
    "    This function loads behavioral data for the motor screening task and formats it to use in this script\n",
    "    \"\"\"\n",
    "    behavioralData = pd.read_csv(filename_behavioral, ',')\n",
    "    \n",
    "    return behavioralData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SyncTriggerPulsesBCI(EEGdata, EEGdevice, fs, behavioralData):\n",
    "    \"\"\"\n",
    "    This function returns the indices for events of interest\n",
    "    \"\"\"\n",
    "    \n",
    "    if EEGdevice == 7:\n",
    "        print('Put code here')\n",
    "    elif EEGdevice == 8:\n",
    "        # Store where the values in trigger are equal to 8 (the audio trigger input channel number)\n",
    "        index_trigger = np.where(EEGdata['STI 014']!=0)\n",
    "        index_trigger = index_trigger[0]\n",
    "\n",
    "        # Number of trials is greater than number of total pulses sent\n",
    "        # 999 when the task ends\n",
    "        move_left_starts = np.where(EEGdata['STI 014'] == 1)[0]\n",
    "        move_right_starts = np.where(EEGdata['STI 014'] == 2)[0]\n",
    "        rest_starts = np.where(EEGdata['STI 014'] == 3)[0]\n",
    "        rest_ends = np.where(EEGdata['STI 014'] == 4)[0]\n",
    "\n",
    "        num_of_trials = len(rest_starts)\n",
    "        num_of_movements = len(move_left_starts) + len(move_right_starts)\n",
    "    \n",
    "    return num_of_trials, num_of_movements, move_left_starts, move_right_starts, rest_starts, rest_ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EpochBCIData(EEGdata, move_left_starts, move_right_starts, rest_starts, rest_ends):\n",
    "    \"\"\"\n",
    "    This function epochs the data\n",
    "    \"\"\"\n",
    "    \n",
    "    if EEGdevice == 7:\n",
    "        channels = EEGdata.columns[1:8]\n",
    "    elif EEGdevice == 8:\n",
    "        channels = EEGdata.columns[0:7]\n",
    "\n",
    "    epochs = []\n",
    "    epochs_norm = []\n",
    "    move_starts = np.sort(np.concatenate((move_left_starts,move_right_starts),0))\n",
    "\n",
    "    for movement in range(0,len(move_starts)):\n",
    "        # Data for this movement\n",
    "        t_start = move_starts[movement] - np.round(1.00*fs)\n",
    "        t_end = move_starts[movement] - np.round(0.250*fs)\n",
    "\n",
    "        # Baseline\n",
    "        restOfInt = np.max(np.where(rest_starts < move_starts[movement]))\n",
    "        tb_start = rest_starts[restOfInt]\n",
    "        tb_end = rest_ends[restOfInt]\n",
    "\n",
    "        baseline = EEGdata.loc[tb_start:tb_end][channels]\n",
    "\n",
    "        # Store epoch\n",
    "        tmp = (EEGdata.loc[t_start:t_end][channels] - np.mean(baseline))/np.std(baseline)\n",
    "        epochs_norm.append(tmp)\n",
    "        epochs.append(EEGdata.loc[t_start:t_end][channels])\n",
    "\n",
    "    return epochs, epochs_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OrganizeTrials(behavioralData):\n",
    "    \"\"\"\n",
    "    Organizes trials\n",
    "    \"\"\"\n",
    "    \n",
    "    # When target was to the left\n",
    "    trialL = np.where(behavioralData['target_x'] < behavioralData['player_x'])\n",
    "    \n",
    "    # When target was to the right\n",
    "    trialR = np.where(behavioralData['target_x'] > behavioralData['player_x'])\n",
    "    \n",
    "    # Create a single list that includes which trial is which (L = 0, R = 1)\n",
    "    trial_type = np.zeros([1,len(behavioralData['score'])])\n",
    "    trial_type[0][trialL] = 0\n",
    "    trial_type[0][trialR] = 1\n",
    "\n",
    "    trial_type = np.round(trial_type[0])\n",
    "\n",
    "    return trial_type, trialL, trialR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractFeaturesBCI(epochs, num_of_movements, channelsToUse, ds_factor):\n",
    "    \"\"\"\n",
    "    Extract signal features of interest\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the summed delta power for each trial\n",
    "    alpha_power = dict.fromkeys(channelsToUse)\n",
    "    beta_power = dict.fromkeys(channelsToUse)\n",
    "    ds_f = ds_factor # downsampling factor\n",
    "\n",
    "    for chanOfInt in channelsToUse:\n",
    "        tmp_alpha = list()\n",
    "        tmp_beta = list()\n",
    "\n",
    "        for movement in range(0, num_of_movements):\n",
    "            f, Pxx_den = signal.welch(signal.decimate(epochs[movement][chanOfInt],ds_f), fs/ds_f, scaling='spectrum')\n",
    "            alpha_idx = np.where(np.logical_and(np.round(f) > 8, np.round(f) <= 12))\n",
    "            tmp_alpha.append(np.sum(Pxx_den[alpha_idx]))\n",
    "\n",
    "            beta_idx = np.where(np.logical_and(np.round(f) > 13, np.round(f) <= 30))\n",
    "            tmp_beta.append(np.sum(Pxx_den[beta_idx]))\n",
    "\n",
    "        alpha_power[chanOfInt] = tmp_alpha\n",
    "        beta_power[chanOfInt] = tmp_beta\n",
    "    \n",
    "    return alpha_power, beta_power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainDecoder(X, y):\n",
    "    \"\"\"\n",
    "    Trains the decoder on ALL the data (does not split into test and train because this is all train)\n",
    "    \"\"\"\n",
    "    # preprocess dataset, split into training and test part\n",
    "    args = np.arange(len(X))\n",
    "    np.random.shuffle(args)\n",
    "    X = [X[i] for i in args]\n",
    "    y = [y[i] for i in args]\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    \n",
    "\n",
    "    # Determine model parameters\n",
    "    activations = ['relu','tanh']\n",
    "    alphas = np.logspace(-6, 3, 10)\n",
    "    solvers = ['lbfgs','sgd']\n",
    "    hyper_params = {\"activation\":activations, \"alpha\":alphas, \"solver\":solvers}\n",
    "    grid = GridSearchCV(MLPClassifier(learning_rate='constant', random_state=1), param_grid=hyper_params, cv=KFold(n_splits=5), verbose=True)\n",
    "    grid.fit(X, y)\n",
    "\n",
    "    # Fit the model\n",
    "    clf = grid.best_estimator_\n",
    "    clf.fit(X,y)\n",
    "    \n",
    "    \"\"\"\n",
    "    # Split into train and test for evaluation\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    # Fit the model\n",
    "    clf = grid.best_estimator_\n",
    "    clf.fit(X_train, y_train)\n",
    "    score = clf.score(X_test, y_test)\n",
    "\n",
    "    print(grid.best_estimator_)\n",
    "    print('-----------')\n",
    "    print('score: ' + str(score))\n",
    "    print(confusion_matrix(y_test, clf.predict(X_test)))\n",
    "    print('-----------')\n",
    "    \"\"\"\n",
    "    \n",
    "    return clf, X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for working with error (ErrPs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EpochErrorData(EEGdata, fs, EEGdevice, t_trial_start):\n",
    "    \"\"\"\n",
    "    This function epochs the data\n",
    "    \"\"\"\n",
    "    if EEGdevice == 7:\n",
    "        channels = EEGdata.columns[1:8]\n",
    "    elif EEGdevice == 8:\n",
    "        channels = EEGdata.columns[0:7]\n",
    "\n",
    "    epochs = []\n",
    "\n",
    "    for trial in range(0,len(t_trial_start)):\n",
    "        t_start = t_trial_start[trial] - np.round(0  * fs)\n",
    "        t_end = t_trial_start[trial] + np.round(0.600 * fs)\n",
    "\n",
    "        # Baseline\n",
    "        tb_start = t_trial_start[trial] - np.round(0.700 * fs)\n",
    "        tb_end = t_trial_start[trial] - np.round(0.100 * fs)\n",
    "        baseline = EEGdata.loc[tb_start:tb_end][channels]\n",
    "\n",
    "        # Store epoch\n",
    "        tmp = (EEGdata.loc[t_start:t_end][channels] - np.mean(baseline))/np.std(baseline)\n",
    "        epochs.append(tmp)\n",
    "    \n",
    "    return epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractErrorFeatures(epochs, num_of_trials, error_template, correct_template, featureType):\n",
    "    \"\"\"\n",
    "    Extract signal features of interest\n",
    "    featureType:    'template' or 'frequency'. 'template' returns features based on the template projection values\n",
    "                    for individual epochs with the error and correct templates. 'frequency' returns features that\n",
    "                    are just delta and theta power for each channel for the epochs\n",
    "    \"\"\"\n",
    "    \n",
    "    if featureType in ['template','Template','TEMPLATE','t','T','projection','Projection','PROJECTION','p','P']:\n",
    "        # template_projection[chanOfInt] will have two columns\n",
    "        # col 1: how well the trial signal matches with the error signal template\n",
    "        # col 2: how well the trial signal matches with the correct signal template\n",
    "        projections_all = dict()\n",
    "        channelsToUse = error_template.keys()\n",
    "\n",
    "        for chanOfInt in channelsToUse:\n",
    "            projections = np.zeros([2, num_of_trials])\n",
    "            for trial in range(0, num_of_trials):\n",
    "                # Individual epoch (normalized)\n",
    "                tmp = epochs_norm[trial][chanOfInt]\n",
    "                a = tmp\n",
    "\n",
    "                # Template waveform for error (normalized)\n",
    "                tmp0 = error_template[chanOfInt]\n",
    "                tmp_norm = (tmp0 - np.mean(tmp0))/np.std(tmp0)\n",
    "                b = tmp_norm\n",
    "\n",
    "                # Template waveform for correct (normalized)\n",
    "                tmp = correct_template[chanOfInt]\n",
    "                tmp_norm = (tmp - np.mean(tmp0))/np.std(tmp0)\n",
    "                c = tmp_norm\n",
    "\n",
    "                # Store sum of convolutions\n",
    "\n",
    "                projections[0][trial] = np.sum(np.convolve(a,b,'same'))\n",
    "                projections[1][trial] = np.sum(np.convolve(a,c,'same'))\n",
    "\n",
    "            projections_all[chanOfInt] = projections\n",
    "        \n",
    "        # Organize the features\n",
    "        channels = list(projections_all.keys())\n",
    "        num_of_features = np.shape(projections_all['Cz'])[0] * len(channels)\n",
    "        channels_full = list(projections_all.keys()) * 2\n",
    "        num_of_trials = np.shape(projections_all['Cz'])[1]\n",
    "\n",
    "        features = np.zeros([num_of_features, num_of_trials])\n",
    "\n",
    "        for trial in range(0, num_of_trials):\n",
    "            # Error trials are 0 to num_of_features//2, and correct trials are num_of_features//2 to num_of_features\n",
    "            for feature in range(0, num_of_features):\n",
    "                features[feature, trial] = projections_all[channels_full[feature]][0][trial]\n",
    "            \n",
    "    elif featureType in ['frequency','Frequency','FREQUENCY','f','F']:\n",
    "        channelsToUse = error_template.keys()\n",
    "        delta_power = dict.fromkeys(channelsToUse)\n",
    "        theta_power = dict.fromkeys(channelsToUse)\n",
    "        ds_f = 1 # downsampling factor\n",
    "\n",
    "        for chanOfInt in channelsToUse:\n",
    "            tmp_delta = list()\n",
    "            tmp_theta = list()\n",
    "\n",
    "            for trial in range(0, num_of_trials):\n",
    "                f, Pxx_den = signal.welch(signal.decimate(epochs_norm[trial][chanOfInt],ds_f), fs/ds_f, scaling='spectrum')\n",
    "                delta_idx = np.where(np.round(f) <= 4)\n",
    "                tmp_delta.append(np.sum(Pxx_den[delta_idx]))\n",
    "\n",
    "                theta_idx = np.where(np.logical_and(np.round(f) > 4, np.round(f) <= 7))\n",
    "                tmp_theta.append(np.sum(Pxx_den[theta_idx]))\n",
    "\n",
    "            delta_power[chanOfInt] = tmp_delta\n",
    "            theta_power[chanOfInt] = tmp_theta\n",
    "            \n",
    "        # Organize the features\n",
    "        num_of_examples = len(delta_power['Cz'])\n",
    "        num_of_features = len(delta_power.keys()) + len(theta_power.keys()) \n",
    "        features = np.zeros([num_of_features, num_of_examples])\n",
    "\n",
    "        # Get all channels in one list to loop through\n",
    "        feature_channels = np.concatenate([np.asarray(list(delta_power.keys())),np.asarray(list(theta_power.keys()))])\n",
    "\n",
    "        for i in range(0, num_of_examples):\n",
    "            for j in range(0, num_of_features//2):\n",
    "                features[j, i] = delta_power[feature_channels[j]][i]\n",
    "            for j in range(num_of_features//2, num_of_features):\n",
    "                features[j, i] = theta_power[feature_channels[j]][i]\n",
    "\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConfidenceScoreExamples(X, y, EEGdata, EEGdevice, fs, num_of_movements, move_starts, trial_type):\n",
    "    \"\"\"\n",
    "    This is the function that does the confidence scoring based on error detection and attention\n",
    "    \"\"\"\n",
    "    # Load the error detection model and see what featureType it used (frequency or template projections)\n",
    "    models = glob.glob(subjID + '_Error_classifier_*')\n",
    "    model_file = models[-1] # load the most recent model\n",
    "    clf_error = pickle.load(open(model_file, 'rb'))\n",
    "    print(model_file)\n",
    "    print(clf_error)\n",
    "\n",
    "    models_data_list = glob.glob(subjID + '_data_for_Error_classifier_*')\n",
    "    models_data = models_data_list[-1] # load the most recent model\n",
    "    loaded_data = np.load(models_data)\n",
    "    featureType = loaded_data['featureType']\n",
    "    \n",
    "    # Load templates if applicable\n",
    "    error_template = loaded_data['error_template'].tolist()\n",
    "    correct_template = loaded_data['correct_template'].tolist()\n",
    "\n",
    "    # Create new epochs for error detection\n",
    "    epochs = EpochErrorData(EEGdata, fs, EEGdevice, move_starts)\n",
    "    features = ExtractErrorFeatures(epochs, num_of_movements, error_template, correct_template, featureType)\n",
    "    features = features.T\n",
    "    \n",
    "    # Scale the features\n",
    "    features = StandardScaler().fit_transform(features)\n",
    "    \n",
    "    # Detect error\n",
    "    preds_error = clf_error.predict(features) # is there an ErrP or not? 1 = yes ErrP, 0 = no ErrP\n",
    "    preds_error_proba = clf_error.predict_proba(features) # what is the prob of there being an ErrP?\n",
    "    \n",
    "    # Confidence in the prediction of error\n",
    "    prob_error = (preds_error_proba[:,1] * preds_error)\n",
    "\n",
    "    # Confidence in the prediction of no error\n",
    "    prob_no_error = preds_error_proba[:,0] * (1-preds_error)\n",
    "\n",
    "    # Confidence the epoch is correct\n",
    "    CS = 1 - (prob_error + prob_no_error)\n",
    "    \n",
    "    \"\"\"\n",
    "    Also return true label stuff\n",
    "    \"\"\"\n",
    "    # Also return true label scores\n",
    "    # Load the error detection model and see what featureType it used (frequency or template projections)\n",
    "    models = glob.glob(subjID + '_MI_classifier_*')\n",
    "    model_file = models[-1] # load the most recent model\n",
    "    clf_MI = pickle.load(open(model_file, 'rb'))\n",
    "\n",
    "    preds_MI = clf_MI.predict(X)\n",
    "    \n",
    "    # trial_type 0 is L, trial_type 1 is R, and TL 1 is high confidence (correct), and TL 0 is low confidence (error)\n",
    "    TL = list()\n",
    "    for trial in range(0, len(trial_type)):\n",
    "        if trial_type[trial] == 0:\n",
    "            if preds_MI[trial] == 0:\n",
    "                TL.append(1)\n",
    "            else:\n",
    "                TL.append(0)\n",
    "        elif trial_type[trial] == 1:\n",
    "            if preds_MI[trial] == 1:\n",
    "                TL.append(1)\n",
    "            else:\n",
    "                TL.append(0)\n",
    "\n",
    "    return CS, TL, preds_MI, preds_error, preds_error_proba, epochs, features, clf_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables to Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjID = 'nile'\n",
    "EEGdevice = 8 # 7 for DSI-7, 8 for Enobio\n",
    "filename_eeg = '../data/Enobio/20190125144918_Nile_BCI.easy'\n",
    "filename_behavioral = '../data/Enobio/BCI_nile_R36.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the BCI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping N/A\n",
      "Creating RawArray with float64 data, n_channels=7, n_times=36504\n",
      "    Range : 0 ... 36503 =      0.000 ...    73.006 secs\n",
      "Ready.\n",
      "Setting up band-pass filter from 1 - 40 Hz\n",
      "l_trans_bandwidth chosen to be 1.0 Hz\n",
      "h_trans_bandwidth chosen to be 10.0 Hz\n",
      "Filter length of 1651 samples (3.302 sec) selected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nilew\\AppData\\Local\\Continuum\\anaconda2\\envs\\py36\\lib\\site-packages\\scipy\\signal\\_arraytools.py:45: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  b = a[a_slice]\n",
      "C:\\Users\\nilew\\AppData\\Local\\Continuum\\anaconda2\\envs\\py36\\lib\\site-packages\\scipy\\signal\\signaltools.py:3463: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return y[sl]\n"
     ]
    }
   ],
   "source": [
    "# Load EEG data\n",
    "EEGdata, fs, fs_accel = LoadEEGData(filename_eeg, EEGdevice)\n",
    "\n",
    "# Load behavioral data\n",
    "behavioralData = LoadBehavioralDataBCI(filename_behavioral)\n",
    "\n",
    "# Sync up trigger pulses\n",
    "num_of_trials, num_of_movements, move_left_starts, move_right_starts, rest_starts, rest_ends = SyncTriggerPulsesBCI(EEGdata, EEGdevice, fs, behavioralData)\n",
    "\n",
    "# Clean the data\n",
    "EEGdata_orig = EEGdata.copy()\n",
    "lf = 1\n",
    "hf = 40\n",
    "\n",
    "if EEGdevice == 7:\n",
    "    channels = EEGdata.columns[1:8]\n",
    "elif EEGdevice == 8:\n",
    "    channels = EEGdata.columns[0:7]\n",
    "\n",
    "# Format our data into an mne-friendly format\n",
    "ch_types = ['eeg']*len(channels)\n",
    "info = mne.create_info(ch_names=list(channels), sfreq=fs, ch_types=ch_types)\n",
    "rawData = EEGdata[channels].values\n",
    "rawData = np.transpose(rawData)\n",
    "raw = mne.io.array.RawArray(rawData, info)\n",
    "raw.set_montage(mne.channels.read_montage(kind='standard_1020'))\n",
    "raw.filter(l_freq=lf, h_freq=hf)\n",
    "\n",
    "# Make a copy of the original data just in case\n",
    "EEGdata[channels] = raw.get_data().T\n",
    "\n",
    "# Epoch the data\n",
    "epochs, epochs_norm = EpochBCIData(EEGdata, move_left_starts, move_right_starts, rest_starts, rest_ends)\n",
    "\n",
    "# Organize trial types\n",
    "trial_type, trialL, trialR = OrganizeTrials(behavioralData)\n",
    "\n",
    "# Get signal features\n",
    "alpha_power, beta_power = ExtractFeaturesBCI(epochs_norm, num_of_movements, ['C3','C4'], 1)\n",
    "motor_features = [alpha_power['C3'], alpha_power['C4'], beta_power['C3'], beta_power['C4']]\n",
    "motor_features = np.transpose(motor_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the MI classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load latest model and its associated data\n",
    "models = glob.glob(subjID + '_MI_classifier_*')\n",
    "model_file = models[-1] # load the most recent model\n",
    "MI_model = pickle.load(open(model_file, 'rb'))\n",
    "\n",
    "models_data_list = glob.glob(subjID + '_data_for_MI_classifier_*')\n",
    "models_data = models_data_list[-1] # load the most recent model\n",
    "MI_data = np.load(models_data)\n",
    "X_loaded_MI = MI_data['X']\n",
    "#X_loaded_MI_pre_scale = MI_data['X_not_scaled']\n",
    "y_loaded_MI = MI_data['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find ErrPs in current BCI data (this function loads the most recent ErrP model for this subject, epochs for error, creates features, and classifies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nile_Error_classifier_2019-01-29-17-15-02.sav\n",
      "MLPClassifier(activation='relu', alpha=0.01, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "       random_state=1, shuffle=True, solver='lbfgs', tol=0.0001,\n",
      "       validation_fraction=0.1, verbose=False, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nilew\\AppData\\Local\\Continuum\\anaconda2\\envs\\py36\\lib\\site-packages\\scipy\\signal\\_arraytools.py:45: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  b = a[a_slice]\n",
      "C:\\Users\\nilew\\AppData\\Local\\Continuum\\anaconda2\\envs\\py36\\lib\\site-packages\\scipy\\signal\\signaltools.py:3463: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return y[sl]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'FC1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda2\\envs\\py36\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3077\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3078\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3079\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'FC1'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-b5b105d80d09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmove_starts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmove_left_starts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmove_right_starts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mCS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds_MI\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds_error\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds_error_proba\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclf_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConfidenceScoreExamples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_new\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_new\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEEGdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEEGdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_of_movements\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmove_starts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrial_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-26-47ed8b23d6d4>\u001b[0m in \u001b[0;36mConfidenceScoreExamples\u001b[1;34m(X, y, EEGdata, EEGdevice, fs, num_of_movements, move_starts, trial_type)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m# Create new epochs for error detection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEpochErrorData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEEGdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEEGdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmove_starts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExtractErrorFeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_of_movements\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror_template\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorrect_template\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatureType\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-846412b787da>\u001b[0m in \u001b[0;36mExtractErrorFeatures\u001b[1;34m(epochs, num_of_trials, error_template, correct_template, featureType)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mtrial\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_of_trials\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPxx_den\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msignal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwelch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecimate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs_norm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchanOfInt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mds_f\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfs\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mds_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaling\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'spectrum'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m                 \u001b[0mdelta_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m                 \u001b[0mtmp_delta\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPxx_den\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdelta_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda2\\envs\\py36\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2686\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2687\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2688\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2690\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda2\\envs\\py36\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2693\u001b[0m         \u001b[1;31m# get column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2694\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2695\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2696\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2697\u001b[0m         \u001b[1;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda2\\envs\\py36\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   2487\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2488\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2489\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2490\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2491\u001b[0m             \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda2\\envs\\py36\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, item, fastpath)\u001b[0m\n\u001b[0;32m   4113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4114\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4115\u001b[1;33m                 \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4116\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4117\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda2\\envs\\py36\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3078\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3079\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3080\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3082\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'FC1'"
     ]
    }
   ],
   "source": [
    "# Choose which examples to keep through confidence scoring\n",
    "X_new = motor_features\n",
    "y_new = trial_type\n",
    "\n",
    "move_starts = np.sort(np.concatenate((move_left_starts,move_right_starts),0))\n",
    "\n",
    "CS, TL, preds_MI, preds_error, preds_error_proba, epochs, features, clf_error = ConfidenceScoreExamples(X_new, y_new, EEGdata, EEGdevice, fs, num_of_movements, move_starts, trial_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh we are running into an issue because the error data and the BCI data here weren't collected with the same montage... And can't use the older BCI data because it doesn't have the proper LSL pulses to indicate rest periods.\n",
    "\n",
    "Re-record the error screening, motor screening, and BCI on yourself tomorrow to test out the pipeline and be able to use this noteobok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See Update_MI_Decoder.ipnyb if you need more tinkering code to copy over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrain and save MI classifier (with pre-scaled features), don't update error classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain\n",
    "adaptationType = 'CS' # either 'CS' for confidence score, or 'TL' for true label\n",
    "threshold = 0.7\n",
    "clf, X, y = RetrainDecoder(MI_model, CS, threshold, X_loaded_MI, y_loaded_MI, motor_features, trial_type, adaptationType)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
