{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM on all classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import specgram\n",
    "from scipy.signal import spectrogram\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_df = pd.read_csv(\"../data/2018-10-12-preston_walk_raw.csv\", skiprows=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>LE</th>\n",
       "      <th>F4</th>\n",
       "      <th>C4</th>\n",
       "      <th>PO8</th>\n",
       "      <th>PO7</th>\n",
       "      <th>C3</th>\n",
       "      <th>F3</th>\n",
       "      <th>Trigger</th>\n",
       "      <th>Time_Offset</th>\n",
       "      <th>ADC_Status</th>\n",
       "      <th>ADC_Sequence</th>\n",
       "      <th>Event</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0033</td>\n",
       "      <td>3642.0</td>\n",
       "      <td>-2536.8</td>\n",
       "      <td>3530.1</td>\n",
       "      <td>2459.4</td>\n",
       "      <td>296.7</td>\n",
       "      <td>3454.5</td>\n",
       "      <td>-1103.4</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>207</td>\n",
       "      <td>0</td>\n",
       "      <td>Start Data Acquisition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0067</td>\n",
       "      <td>3635.7</td>\n",
       "      <td>-2526.3</td>\n",
       "      <td>3531.0</td>\n",
       "      <td>2451.9</td>\n",
       "      <td>299.7</td>\n",
       "      <td>3459.6</td>\n",
       "      <td>-1098.9</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>208</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>3632.7</td>\n",
       "      <td>-2535.9</td>\n",
       "      <td>3528.6</td>\n",
       "      <td>2441.4</td>\n",
       "      <td>302.1</td>\n",
       "      <td>3454.8</td>\n",
       "      <td>-1103.4</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>209</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0133</td>\n",
       "      <td>3626.7</td>\n",
       "      <td>-2535.3</td>\n",
       "      <td>3524.1</td>\n",
       "      <td>2450.1</td>\n",
       "      <td>297.9</td>\n",
       "      <td>3455.1</td>\n",
       "      <td>-1097.1</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>210</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0167</td>\n",
       "      <td>3626.4</td>\n",
       "      <td>-2527.8</td>\n",
       "      <td>3539.4</td>\n",
       "      <td>2469.0</td>\n",
       "      <td>289.8</td>\n",
       "      <td>3452.7</td>\n",
       "      <td>-1089.9</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>211</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Time      LE      F4      C4     PO8    PO7      C3      F3  Trigger  \\\n",
       "0  0.0033  3642.0 -2536.8  3530.1  2459.4  296.7  3454.5 -1103.4        0   \n",
       "1  0.0067  3635.7 -2526.3  3531.0  2451.9  299.7  3459.6 -1098.9        0   \n",
       "2  0.0100  3632.7 -2535.9  3528.6  2441.4  302.1  3454.8 -1103.4        0   \n",
       "3  0.0133  3626.7 -2535.3  3524.1  2450.1  297.9  3455.1 -1097.1        0   \n",
       "4  0.0167  3626.4 -2527.8  3539.4  2469.0  289.8  3452.7 -1089.9        0   \n",
       "\n",
       "   Time_Offset  ADC_Status  ADC_Sequence  Event                Comments  \n",
       "0            6           0           207      0  Start Data Acquisition  \n",
       "1           12           0           208      0                     NaN  \n",
       "2           18           0           209      0                     NaN  \n",
       "3           24           0           210      0                     NaN  \n",
       "4           30           0           211      0                     NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eeg_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne.filter import filter_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = eeg_df.columns[1:8] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_data = eeg_df[channels].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up band-pass filter from 1 - 40 Hz\n",
      "l_trans_bandwidth chosen to be 1.0 Hz\n",
      "h_trans_bandwidth chosen to be 10.0 Hz\n",
      "Filter length of 991 samples (3.303 sec) selected\n"
     ]
    }
   ],
   "source": [
    "filtered = filter_data(eeg_data.T, sfreq=300, l_freq=1, h_freq=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_df[channels] = filtered.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epoch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = eeg_df.Event.nonzero()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1446,   2970,   7952,  47145,  78073, 135086])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_one_start = events[2] + 300\n",
    "walk_one_stop = events[3] - 300\n",
    "typing_start = events[3] + 300\n",
    "typing_stop = events[4] - 300\n",
    "talking_start = events[4] + 300  \n",
    "talking_stop = events[5] - 300  \n",
    "walk_two_start = events[5] + 300\n",
    "walk_two_stop = len(eeg_df) - 9000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "typing_data = eeg_df.loc[typing_start : typing_stop][channels]\n",
    "talking_data = eeg_df.loc[talking_start : talking_stop][channels]\n",
    "walk_one_data = eeg_df.loc[walk_one_start : walk_one_stop][channels]\n",
    "walk_two_data = eeg_df.loc[walk_two_start : walk_two_stop][channels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "typing_data = typing_data[: len(typing_data) // 300 * 300]\n",
    "talking_data = talking_data[: len(talking_data) // 300 * 300]\n",
    "walk_one_data = walk_one_data[: len(walk_one_data) // 300 * 300]\n",
    "walk_two_data = walk_two_data[: len(walk_two_data) // 300 * 300]\n",
    "walk_data = np.concatenate((walk_one_data, walk_two_data), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize the data\n",
    "Normalize the data for each channel (for each epoch) such that it has mean zero and unit variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_one_normalized = (walk_one_data[channels] - np.mean(walk_one_data[channels]))/np.std(walk_one_data[channels])\n",
    "typing_normalized = (typing_data[channels] - np.mean(typing_data[channels]))/np.std(typing_data[channels])\n",
    "talking_normalized = (talking_data[channels] - np.mean(talking_data[channels]))/np.std(talking_data[channels])\n",
    "walk_two_normalized = (walk_two_data[channels] - np.mean(walk_two_data[channels]))/np.std(walk_two_data[channels])\n",
    "walk_normalized = np.concatenate((walk_one_normalized, walk_two_normalized), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power, create data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.concatenate((typing_normalized, talking_normalized, walk_normalized), axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, t, typing_spec = spectrogram(typing_normalized.T, fs=300, nperseg=600, noverlap=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "typing_spec = typing_spec[:, :81, :].reshape(-1, typing_spec.shape[2]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, t, talking_spec = spectrogram(talking_normalized.T, fs=300, nperseg=600, noverlap=300)\n",
    "talking_spec = talking_spec[:, :81, :].reshape(-1, talking_spec.shape[2]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, t, walk_spec = spectrogram(walk_normalized.T, fs=300, nperseg=600, noverlap=300)\n",
    "walk_spec = walk_spec[:, :81, :].reshape(-1, walk_spec.shape[2]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((typing_spec, talking_spec, walk_spec), axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.concatenate((np.zeros(len(typing_spec)), np.ones(len(talking_spec)), np.full((len(walk_spec), ), 2)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle data to get train/test\n",
    "args = np.arange(len(X))\n",
    "np.random.shuffle(args)\n",
    "X = X[args]\n",
    "y = y[args]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = [0.001, 0.01, 0.1, 1, 5, 10]\n",
    "degrees = [1, 5, 10, 15, 20, 50, 100, 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params = {\"C\":Cs, \"degree\":degrees}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(SVC(kernel='poly'), param_grid=hyper_params, cv=KFold(n_splits=5), verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 240 out of 240 | elapsed:   44.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=5, random_state=None, shuffle=False),\n",
       "       error_score='raise',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='poly',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [0.001, 0.01, 0.1, 1, 5, 10], 'degree': [1, 5, 10, 15, 20, 50, 100, 500]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=True)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=1, gamma='auto', kernel='poly',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(kernel='poly', C=10, degree=1, verbose=True) # C=0.1 without normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=1, gamma='auto', kernel='poly',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.698237885462555"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7254901960784313"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "hids = [(100, ), (200, ), (250, ), (400, ), (500, ), (100, 50), (200, 100), (500, 100)]\n",
    "alphas = [0.1, 0.01, 0.001, 0.0001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(MLPClassifier(tol=0.001), param_grid={'hidden_layer_sizes':hids, 'alpha':alphas}, cv=KFold(n_splits=10), verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/miniconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done 320 out of 320 | elapsed:  7.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=10, random_state=None, shuffle=False),\n",
       "       error_score='raise',\n",
       "       estimator=MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'hidden_layer_sizes': [(100,), (200,), (250,), (400,), (500,), (100, 50), (200, 100), (500, 100)], 'alpha': [0.1, 0.01, 0.001, 0.0001]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=True)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(200, 100), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_net = MLPClassifier(hidden_layer_sizes=(200, 100), alpha=0.0001, verbose=True, tol=0.001) #alpha=0.01, hidden_layer_sizes=(250, ) not normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.08642566\n",
      "Iteration 2, loss = 0.97606894\n",
      "Iteration 3, loss = 0.90002606\n",
      "Iteration 4, loss = 0.83854503\n",
      "Iteration 5, loss = 0.78024983\n",
      "Iteration 6, loss = 0.73305564\n",
      "Iteration 7, loss = 0.68939757\n",
      "Iteration 8, loss = 0.65029928\n",
      "Iteration 9, loss = 0.61765090\n",
      "Iteration 10, loss = 0.58996266\n",
      "Iteration 11, loss = 0.56512687\n",
      "Iteration 12, loss = 0.54214376\n",
      "Iteration 13, loss = 0.52197727\n",
      "Iteration 14, loss = 0.50371281\n",
      "Iteration 15, loss = 0.48787415\n",
      "Iteration 16, loss = 0.47248644\n",
      "Iteration 17, loss = 0.45959413\n",
      "Iteration 18, loss = 0.44438399\n",
      "Iteration 19, loss = 0.43311427\n",
      "Iteration 20, loss = 0.42126374\n",
      "Iteration 21, loss = 0.41300282\n",
      "Iteration 22, loss = 0.40085160\n",
      "Iteration 23, loss = 0.39400903\n",
      "Iteration 24, loss = 0.37784669\n",
      "Iteration 25, loss = 0.37025894\n",
      "Iteration 26, loss = 0.35923571\n",
      "Iteration 27, loss = 0.35009986\n",
      "Iteration 28, loss = 0.34136261\n",
      "Iteration 29, loss = 0.33019423\n",
      "Iteration 30, loss = 0.32652379\n",
      "Iteration 31, loss = 0.31461762\n",
      "Iteration 32, loss = 0.30453307\n",
      "Iteration 33, loss = 0.29619419\n",
      "Iteration 34, loss = 0.28686285\n",
      "Iteration 35, loss = 0.27851453\n",
      "Iteration 36, loss = 0.26927893\n",
      "Iteration 37, loss = 0.26225451\n",
      "Iteration 38, loss = 0.25293557\n",
      "Iteration 39, loss = 0.24488559\n",
      "Iteration 40, loss = 0.23644845\n",
      "Iteration 41, loss = 0.22776366\n",
      "Iteration 42, loss = 0.22078179\n",
      "Iteration 43, loss = 0.21217799\n",
      "Iteration 44, loss = 0.20391803\n",
      "Iteration 45, loss = 0.19692700\n",
      "Iteration 46, loss = 0.18853339\n",
      "Iteration 47, loss = 0.18142778\n",
      "Iteration 48, loss = 0.17353490\n",
      "Iteration 49, loss = 0.16627809\n",
      "Iteration 50, loss = 0.15820941\n",
      "Iteration 51, loss = 0.15183836\n",
      "Iteration 52, loss = 0.14515421\n",
      "Iteration 53, loss = 0.13684992\n",
      "Iteration 54, loss = 0.13103719\n",
      "Iteration 55, loss = 0.12504175\n",
      "Iteration 56, loss = 0.11884752\n",
      "Iteration 57, loss = 0.11360140\n",
      "Iteration 58, loss = 0.10713336\n",
      "Iteration 59, loss = 0.10135936\n",
      "Iteration 60, loss = 0.09964290\n",
      "Iteration 61, loss = 0.09688789\n",
      "Iteration 62, loss = 0.13640123\n",
      "Iteration 63, loss = 0.09930814\n",
      "Iteration 64, loss = 0.10247683\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(200, 100), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.001, validation_fraction=0.1,\n",
       "       verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_net.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9955947136563876"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_net.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9411764705882353"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_net.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7,  3,  0],\n",
       "       [ 0, 22,  1],\n",
       "       [ 0,  0, 18]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, best_net.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
