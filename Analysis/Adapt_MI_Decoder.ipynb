{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adapt Motor Imagery classifier with confidence scores\n",
    "Use this clean notebook to create the code to update the MI classifier using confidence scores created from Error-related Potentials (ErrPs) and Default Mode Network (DMN) activity / attention.\n",
    "\n",
    "Please see `Check_BCI_ErrP_with_Attention.ipnyb` for a version of this code with plotting / checks / tinkering\n",
    "\n",
    "Nile Wilson 2019.01.31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.fftpack import fft, ifft\n",
    "from scipy import signal\n",
    "from mne.filter import filter_data\n",
    "\n",
    "import scipy.signal as scisig\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import pickle\n",
    "import glob\n",
    "import csv\n",
    "import mne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WARNING\n",
    "Make sure to change the filepath search locations when making this a function! It currently looks for stuff in the same folder or ../data/nile/, but should be savedata (check capitalization) in the end!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for working with the BCI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadEEGData(filename, EEGdevice):\n",
    "    \"\"\" This function converts a single .easy file (from NIC2) to an easy-to-use dataframe.\n",
    "    Uses both the .easy file and .info file (containing metadata)\n",
    "    \n",
    "    ---- Input ----\n",
    "    filename: string containing the .easy filepath\n",
    "    \n",
    "    ---- Output ----\n",
    "    df: dataframe containing all the EEG, accelerometer, and event marker data\n",
    "    fs: sampling rate for the EEG data (Hz)\n",
    "    fs_accel: sampling rate for the accelerometer data (Hz)\n",
    "    \n",
    "    \"\"\"\n",
    "    if EEGdevice == 7:\n",
    "        x = 1\n",
    "    elif EEGdevice == 8:\n",
    "        # Read in the .easy file\n",
    "        df = pd.read_csv(filename, delimiter='\\t', header=None)\n",
    "\n",
    "        # Get metadata from the .info file\n",
    "        fname = filename[:-5] + '.info'\n",
    "        with open(fname) as f:\n",
    "            content = f.readlines()\n",
    "        content = [x.strip() for x in content]\n",
    "\n",
    "        # Get the channel names\n",
    "        channel_info = [x for x in content if 'Channel ' in x]\n",
    "        channel_names = []\n",
    "        for ch in range(len(channel_info)):\n",
    "            channel_names.append(channel_info[ch].split(': ')[1])\n",
    "\n",
    "        channel_names.append('X')\n",
    "        channel_names.append('Y')\n",
    "        channel_names.append('Z')\n",
    "        channel_names.append('STI 014')\n",
    "        channel_names.append('DateTime')\n",
    "\n",
    "        # Get sampling rates\n",
    "        sampling_rates = [x for x in content if 'sampling rate: ' in x]\n",
    "        fs_all = []\n",
    "        for freq in range(len(sampling_rates)):\n",
    "            tmp = sampling_rates[freq].split(': ')[1].split(' ')[0]\n",
    "            if tmp in ['N/A']:\n",
    "                print('Skipping N/A')\n",
    "            else:\n",
    "                fs_all.append(float(sampling_rates[freq].split(': ')[1].split(' ')[0]))\n",
    "\n",
    "        # Store sampling rates\n",
    "        fs = fs_all[0]\n",
    "        fs_accel = fs_all[1]\n",
    "\n",
    "        # Assign the column names\n",
    "        df.columns = channel_names\n",
    "    \n",
    "    # Return dataframe and sampling rates\n",
    "    return df, fs, fs_accel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadBehavioralDataBCI(filename_behavioral):\n",
    "    \"\"\"\n",
    "    This function loads behavioral data for the motor screening task and formats it to use in this script\n",
    "    \"\"\"\n",
    "    behavioralData = pd.read_csv(filename_behavioral, ',')\n",
    "    \n",
    "    return behavioralData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SyncTriggerPulsesBCI(EEGdata, EEGdevice, fs, behavioralData):\n",
    "    \"\"\"\n",
    "    This function returns the indices for events of interest\n",
    "    \"\"\"\n",
    "    \n",
    "    if EEGdevice == 7:\n",
    "        print('Put code here')\n",
    "    elif EEGdevice == 8:\n",
    "        # Store where the values in trigger are equal to 8 (the audio trigger input channel number)\n",
    "        index_trigger = np.where(EEGdata['STI 014']!=0)\n",
    "        index_trigger = index_trigger[0]\n",
    "\n",
    "        # Number of trials is greater than number of total pulses sent\n",
    "        # 999 when the task ends\n",
    "        move_left_starts = np.where(EEGdata['STI 014'] == 1)[0]\n",
    "        move_right_starts = np.where(EEGdata['STI 014'] == 2)[0]\n",
    "        rest_starts = np.where(EEGdata['STI 014'] == 3)[0]\n",
    "        rest_ends = np.where(EEGdata['STI 014'] == 4)[0]\n",
    "        \n",
    "        # If the number of rest_starts and rest_ends don't match, drop the extra one\n",
    "        # there should, by default, only be 12 starts and 12 ends\n",
    "\n",
    "        if len(rest_ends) > len(rest_starts):\n",
    "            if rest_ends[0] < rest_starts[0]:\n",
    "                rest_ends = rest_ends[1:]\n",
    "        elif len(rest_ends) < len(rest_starts):\n",
    "            if rest_ends[0] > rest_starts[0]:\n",
    "                rest_starts = rest_starts[1:]\n",
    "        \n",
    "        move_starts = np.sort(np.concatenate((move_left_starts,move_right_starts),0))\n",
    "        total_movements = len(move_starts)\n",
    "\n",
    "        # exclude movements that occur without defined baseline (if you need to get rid of first rest)\n",
    "        hasBaseline = list()\n",
    "        for movement in range(0,len(move_starts)):\n",
    "            hasBaseline.append(True in (rest_starts < move_starts[movement]))\n",
    "\n",
    "        np.where(hasBaseline)\n",
    "        move_starts = move_starts[np.where(hasBaseline)]\n",
    "\n",
    "        # exclude the move lefts and move rights that were thrown out in move_starts\n",
    "        for movement in range(0,total_movements):\n",
    "            if hasBaseline[movement] is False:\n",
    "                # for the left movements\n",
    "                idx_left = np.where(move_left_starts == move_starts[movement])\n",
    "                idx_left = np.asarray(idx_left)\n",
    "                idx_right = np.where(move_right_starts == move_starts[movement])\n",
    "                idx_right = np.asarray(idx_right)\n",
    "\n",
    "                if idx_left.size > 0:\n",
    "                    move_left_starts = np.delete(move_left_starts, idx_left)\n",
    "                if idx_right.size > 0:\n",
    "                    move_right_starts = np.delete(move_right_starts, idx_right)\n",
    "                \n",
    "        num_of_trials = len(rest_starts)\n",
    "        num_of_movements = len(move_left_starts) + len(move_right_starts)\n",
    "    \n",
    "    return num_of_trials, num_of_movements, move_starts, hasBaseline, rest_starts, rest_ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EpochBCIData(EEGdata, move_starts, rest_starts, rest_ends):\n",
    "    \"\"\"\n",
    "    This function epochs the data\n",
    "    \"\"\"\n",
    "    \n",
    "    if EEGdevice == 7:\n",
    "        channels = EEGdata.columns[1:8]\n",
    "    elif EEGdevice == 8:\n",
    "        channels = EEGdata.columns[0:8]\n",
    "\n",
    "    epochs = []\n",
    "    epochs_norm = []\n",
    "\n",
    "    for movement in range(0,len(move_starts)):\n",
    "        # Data for this movement\n",
    "        t_start = move_starts[movement] - np.round(1.00*fs)\n",
    "        t_end = move_starts[movement] - np.round(0.250*fs)\n",
    "\n",
    "        # Baseline\n",
    "        restOfInt = np.max(np.where(rest_starts < move_starts[movement]))\n",
    "        tb_start = rest_starts[restOfInt]\n",
    "        tb_end = rest_ends[restOfInt]\n",
    "\n",
    "        baseline = EEGdata.loc[tb_start:tb_end][channels]\n",
    "\n",
    "        # Store epoch\n",
    "        tmp = (EEGdata.loc[t_start:t_end][channels] - np.mean(baseline))/np.std(baseline)\n",
    "        epochs_norm.append(tmp)\n",
    "        epochs.append(EEGdata.loc[t_start:t_end][channels])\n",
    "\n",
    "    return epochs, epochs_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OrganizeTrials(behavioralData, hasBaseline):\n",
    "    \"\"\"\n",
    "    Organizes trials\n",
    "    \"\"\"\n",
    "    \n",
    "    # When target was to the left\n",
    "    trialL = np.where(behavioralData['target_x'] < behavioralData['player_x'])\n",
    "    \n",
    "    # When target was to the right\n",
    "    trialR = np.where(behavioralData['target_x'] > behavioralData['player_x'])\n",
    "    \n",
    "    # Create a single list that includes which trial is which (L = 0, R = 1)\n",
    "    trial_type = np.zeros([1,len(behavioralData['score'])])\n",
    "    trial_type[0][trialL] = 0\n",
    "    trial_type[0][trialR] = 1\n",
    "\n",
    "    trial_type = np.round(trial_type[0])\n",
    "    \n",
    "    # Remove trials if no baseline\n",
    "    for movement in range(0,len(hasBaseline)):\n",
    "        if hasBaseline[movement] is False:\n",
    "            trial_type = np.delete(trial_type, movement)\n",
    "            \n",
    "    return trial_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractFeaturesBCI(epochs, num_of_movements, channelsToUse, ds_factor):\n",
    "    \"\"\"\n",
    "    Extract signal features of interest\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the summed delta power for each trial\n",
    "    alpha_power = dict.fromkeys(channelsToUse)\n",
    "    beta_power = dict.fromkeys(channelsToUse)\n",
    "    ds_f = ds_factor # downsampling factor\n",
    "\n",
    "    for chanOfInt in channelsToUse:\n",
    "        tmp_alpha = list()\n",
    "        tmp_beta = list()\n",
    "\n",
    "        for movement in range(0, num_of_movements):\n",
    "            f, Pxx_den = signal.welch(signal.decimate(epochs[movement][chanOfInt],ds_f), fs/ds_f, scaling='spectrum')\n",
    "            alpha_idx = np.where(np.logical_and(np.round(f) > 8, np.round(f) <= 12))\n",
    "            tmp_alpha.append(np.sum(Pxx_den[alpha_idx]))\n",
    "\n",
    "            beta_idx = np.where(np.logical_and(np.round(f) > 13, np.round(f) <= 30))\n",
    "            tmp_beta.append(np.sum(Pxx_den[beta_idx]))\n",
    "\n",
    "        alpha_power[chanOfInt] = tmp_alpha\n",
    "        beta_power[chanOfInt] = tmp_beta\n",
    "    \n",
    "    return alpha_power, beta_power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for working with error (ErrPs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EpochErrorData(EEGdata, fs, EEGdevice, t_trial_start):\n",
    "    \"\"\"\n",
    "    This function epochs the data\n",
    "    \"\"\"\n",
    "    if EEGdevice == 7:\n",
    "        channels = EEGdata.columns[1:8]\n",
    "    elif EEGdevice == 8:\n",
    "        channels = EEGdata.columns[0:8]\n",
    "\n",
    "    epochs = []\n",
    "\n",
    "    for trial in range(0,len(t_trial_start)):\n",
    "        t_start = t_trial_start[trial] - np.round(0  * fs)\n",
    "        t_end = t_trial_start[trial] + np.round(0.600 * fs)\n",
    "\n",
    "        # Baseline\n",
    "        tb_start = t_trial_start[trial] - np.round(0.700 * fs)\n",
    "        tb_end = t_trial_start[trial] - np.round(0.100 * fs)\n",
    "        baseline = EEGdata.loc[tb_start:tb_end][channels]\n",
    "\n",
    "        # Store epoch\n",
    "        tmp = (EEGdata.loc[t_start:t_end][channels] - np.mean(baseline))/np.std(baseline)\n",
    "        epochs.append(tmp)\n",
    "    \n",
    "    return epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractErrorFeatures(epochs, num_of_trials, error_template, correct_template, featureType):\n",
    "    \"\"\"\n",
    "    Extract signal features of interest\n",
    "    featureType:    'template' or 'frequency'. 'template' returns features based on the template projection values\n",
    "                    for individual epochs with the error and correct templates. 'frequency' returns features that\n",
    "                    are just delta and theta power for each channel for the epochs\n",
    "    \"\"\"\n",
    "    \n",
    "    if featureType in ['template','Template','TEMPLATE','t','T','projection','Projection','PROJECTION','p','P']:\n",
    "        # template_projection[chanOfInt] will have two columns\n",
    "        # col 1: how well the trial signal matches with the error signal template\n",
    "        # col 2: how well the trial signal matches with the correct signal template\n",
    "        projections_all = dict()\n",
    "        channelsToUse = error_template.keys()\n",
    "\n",
    "        for chanOfInt in channelsToUse:\n",
    "            projections = np.zeros([2, num_of_trials])\n",
    "            for trial in range(0, num_of_trials):\n",
    "                # Individual epoch (normalized)\n",
    "                tmp = epochs_norm[trial][chanOfInt]\n",
    "                a = tmp\n",
    "\n",
    "                # Template waveform for error (normalized)\n",
    "                tmp0 = error_template[chanOfInt]\n",
    "                tmp_norm = (tmp0 - np.mean(tmp0))/np.std(tmp0)\n",
    "                b = tmp_norm\n",
    "\n",
    "                # Template waveform for correct (normalized)\n",
    "                tmp = correct_template[chanOfInt]\n",
    "                tmp_norm = (tmp - np.mean(tmp0))/np.std(tmp0)\n",
    "                c = tmp_norm\n",
    "\n",
    "                # Store sum of convolutions\n",
    "\n",
    "                projections[0][trial] = np.sum(np.convolve(a,b,'same'))\n",
    "                projections[1][trial] = np.sum(np.convolve(a,c,'same'))\n",
    "\n",
    "            projections_all[chanOfInt] = projections\n",
    "        \n",
    "        # Organize the features\n",
    "        channels = list(projections_all.keys())\n",
    "        num_of_features = np.shape(projections_all['Cz'])[0] * len(channels)\n",
    "        channels_full = list(projections_all.keys()) * 2\n",
    "        num_of_trials = np.shape(projections_all['Cz'])[1]\n",
    "\n",
    "        features = np.zeros([num_of_features, num_of_trials])\n",
    "\n",
    "        for trial in range(0, num_of_trials):\n",
    "            # Error trials are 0 to num_of_features//2, and correct trials are num_of_features//2 to num_of_features\n",
    "            for feature in range(0, num_of_features):\n",
    "                features[feature, trial] = projections_all[channels_full[feature]][0][trial]\n",
    "            \n",
    "    elif featureType in ['frequency','Frequency','FREQUENCY','f','F']:\n",
    "        channelsToUse = error_template.keys()\n",
    "        delta_power = dict.fromkeys(channelsToUse)\n",
    "        theta_power = dict.fromkeys(channelsToUse)\n",
    "        ds_f = 1 # downsampling factor\n",
    "\n",
    "        for chanOfInt in channelsToUse:\n",
    "            tmp_delta = list()\n",
    "            tmp_theta = list()\n",
    "\n",
    "            for trial in range(0, num_of_trials):\n",
    "                f, Pxx_den = signal.welch(signal.decimate(epochs_norm[trial][chanOfInt],ds_f), fs/ds_f, scaling='spectrum')\n",
    "                delta_idx = np.where(np.round(f) <= 4)\n",
    "                tmp_delta.append(np.sum(Pxx_den[delta_idx]))\n",
    "\n",
    "                theta_idx = np.where(np.logical_and(np.round(f) > 4, np.round(f) <= 7))\n",
    "                tmp_theta.append(np.sum(Pxx_den[theta_idx]))\n",
    "\n",
    "            delta_power[chanOfInt] = tmp_delta\n",
    "            theta_power[chanOfInt] = tmp_theta\n",
    "            \n",
    "        # Organize the features\n",
    "        num_of_examples = len(delta_power['Cz'])\n",
    "        num_of_features = len(delta_power.keys()) + len(theta_power.keys()) \n",
    "        features = np.zeros([num_of_features, num_of_examples])\n",
    "\n",
    "        # Get all channels in one list to loop through\n",
    "        feature_channels = np.concatenate([np.asarray(list(delta_power.keys())),np.asarray(list(theta_power.keys()))])\n",
    "\n",
    "        for i in range(0, num_of_examples):\n",
    "            for j in range(0, num_of_features//2):\n",
    "                features[j, i] = delta_power[feature_channels[j]][i]\n",
    "            for j in range(num_of_features//2, num_of_features):\n",
    "                features[j, i] = theta_power[feature_channels[j]][i]\n",
    "\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConfidenceScoreExamples(X, y, EEGdata, epochs_norm, EEGdevice, fs, num_of_movements, move_starts, trial_type):\n",
    "    \"\"\"\n",
    "    This is the function that does the confidence scoring based on error detection and attention\n",
    "    \"\"\"\n",
    "    # Load the error detection model and see what featureType it used (frequency or template projections)\n",
    "    models = glob.glob('Models/' + subjID + '_Error_classifier_*')\n",
    "    model_file = models[-1] # load the most recent model\n",
    "    clf_error = pickle.load(open(model_file, 'rb'))\n",
    "    print(model_file)\n",
    "    print(clf_error)\n",
    "\n",
    "    models_data_list = glob.glob('Models/' + subjID + '_data_for_Error_classifier_*')\n",
    "    models_data = models_data_list[-1] # load the most recent model\n",
    "    loaded_data = np.load(models_data)\n",
    "    featureType = loaded_data['featureType']\n",
    "    \n",
    "    # Load templates if applicable\n",
    "    error_template = loaded_data['error_template'].tolist()\n",
    "    correct_template = loaded_data['correct_template'].tolist()\n",
    "\n",
    "    # Create new epochs for error detection\n",
    "    epochs = EpochErrorData(EEGdata, fs, EEGdevice, move_starts)\n",
    "    features = ExtractErrorFeatures(epochs, num_of_movements, error_template, correct_template, featureType)\n",
    "    features = features.T\n",
    "    \n",
    "    # Scale the features\n",
    "    features = StandardScaler().fit_transform(features)\n",
    "    \n",
    "    # Detect error\n",
    "    preds_error = clf_error.predict(features) # is there an ErrP or not? 1 = yes ErrP, 0 = no ErrP\n",
    "    preds_error_proba = clf_error.predict_proba(features) # what is the prob of there being an ErrP?\n",
    "    \n",
    "    # Confidence in the prediction of error\n",
    "    prob_error = (preds_error_proba[:,1] * preds_error)\n",
    "\n",
    "    # Confidence in the prediction of no error\n",
    "    prob_no_error = preds_error_proba[:,0] * (1-preds_error)\n",
    "    \n",
    "    # Get values for attention\n",
    "    attention_alpha, attention_beta = ExtractFeaturesBCI(epochs_norm, num_of_movements, ['Pz'], 1)\n",
    "    epochs_rest, epochs_rest_norm = GetRestEpochsBCI(EEGdata, rest_starts, rest_ends)\n",
    "    attention_alpha_rest_norm, attention_beta_rest_norm = ExtractFeaturesBCI(epochs_rest_norm, len(epochs_rest), ['Pz'], 1)\n",
    "    beta_threshold = np.mean(attention_beta_rest_norm['Pz'])-(2*np.std(attention_beta_rest_norm['Pz']))\n",
    "    distance_from_threshold = (attention_beta['Pz']-beta_threshold)/np.max(attention_beta_rest_norm['Pz'])\n",
    "\n",
    "    # Confidence the epoch is correct\n",
    "    w_a = 0.5\n",
    "    CS_pre_scale = ((prob_no_error - prob_error + 1) - (w_a*distance_from_threshold))/2\n",
    "    CS = (CS_pre_scale - np.min(CS_pre_scale))/(np.max(CS_pre_scale)-np.min(CS_pre_scale))\n",
    "    \n",
    "    \"\"\"\n",
    "    Also return true label stuff\n",
    "    \"\"\"\n",
    "    # Also return true label scores\n",
    "    # Load the error detection model and see what featureType it used (frequency or template projections)\n",
    "    models = glob.glob('Models/' + subjID + '_MI_classifier_*')\n",
    "    model_file = models[-1] # load the most recent model\n",
    "    clf_MI = pickle.load(open(model_file, 'rb'))\n",
    "\n",
    "    preds_MI = clf_MI.predict(X)\n",
    "    \n",
    "    # trial_type 0 is L, trial_type 1 is R, and TL 1 is high confidence (correct), and TL 0 is low confidence (error)\n",
    "    TL = list()\n",
    "    for trial in range(0, len(trial_type)):\n",
    "        if trial_type[trial] == 0:\n",
    "            if preds_MI[trial] == 0:\n",
    "                TL.append(1)\n",
    "            else:\n",
    "                TL.append(0)\n",
    "        elif trial_type[trial] == 1:\n",
    "            if preds_MI[trial] == 1:\n",
    "                TL.append(1)\n",
    "            else:\n",
    "                TL.append(0)\n",
    "\n",
    "    return CS, TL, preds_MI, preds_error, preds_error_proba, epochs, features, clf_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RetrainDecoder(clf, CS, threshold, X_old, y_old, X_new, y_new, adaptationType):\n",
    "    \"\"\"\n",
    "    Retrains the decoder on ALL the previous data plus what you decided to add in with high confidence scores\n",
    "    \n",
    "    clf: your preloaded model (most recent version)\n",
    "    CS: confidence scores (if true labels, use TL instead of CS)\n",
    "    X_old: your preloaded model data (X)\n",
    "    y_old: your preloaded model data (y)\n",
    "    X_new: your new motor features\n",
    "    y_new: your new labels\n",
    "    \"\"\"\n",
    "    # Concatenate old X and y with new X and y that have a high enough confidence score\n",
    "    aboveThreshold = np.where(CS>threshold)\n",
    "    \n",
    "    X = np.concatenate((X_old, X_new[aboveThreshold]), axis=0)\n",
    "    y = np.concatenate((y_old, np.asarray(y_new)[aboveThreshold]), axis=0)\n",
    "    \n",
    "    # preprocess dataset, split into training and test part\n",
    "    args = np.arange(len(X))\n",
    "    np.random.shuffle(args)\n",
    "    X = [X[i] for i in args]\n",
    "    y = [y[i] for i in args]\n",
    "    X_not_scaled = X\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # Fit the model\n",
    "    clf.fit(X,y)\n",
    "    \n",
    "    \"\"\"\n",
    "    # Split into train and test for evaluation\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    # Fit the model\n",
    "    clf = grid.best_estimator_\n",
    "    clf.fit(X_train, y_train)\n",
    "    score = clf.score(X_test, y_test)\n",
    "\n",
    "    print(grid.best_estimator_)\n",
    "    print('-----------')\n",
    "    print('score: ' + str(score))\n",
    "    print(confusion_matrix(y_test, clf.predict(X_test)))\n",
    "    print('-----------')\n",
    "    \"\"\"\n",
    "    \n",
    "    return clf, X, X_not_scaled, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SaveDecoderAndData(clf, X, X_not_scaled, y, subjID):\n",
    "    \"\"\"\n",
    "    Save the decoder and the data it was trained/tested on\n",
    "    \"\"\"\n",
    "    time_to_save = datetime.datetime.now().isoformat()\n",
    "    time_to_save = time_to_save.replace('T','-')\n",
    "    time_to_save = time_to_save.replace(':','-')\n",
    "    \n",
    "    model = clf\n",
    "    model_file = 'Models/' + subjID + '_MI_classifier_' + time_to_save[:19] + '.sav'\n",
    "    pickle.dump(model, open(model_file, 'wb'))\n",
    "    \n",
    "    filepath_export_data = 'Models/' + subjID + '_data_for_MI_classifier_' + time_to_save[:19] + '.npz'\n",
    "    np.savez_compressed(filepath_export_data, X=X, X_not_scaled=X_not_scaled, y=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to work with attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetRestEpochsBCI(EEGdata, rest_starts, rest_ends):\n",
    "    \"\"\"\n",
    "    This function returns the rest (ITI) epochs that were used for baseline normalization in the BCI\n",
    "    \"\"\" \n",
    "    if EEGdevice == 7:\n",
    "        channels = EEGdata.columns[1:8]\n",
    "    elif EEGdevice == 8:\n",
    "        channels = EEGdata.columns[0:8]\n",
    "\n",
    "    epochs = []\n",
    "    epochs_norm = []\n",
    "\n",
    "    for rest in range(0,len(rest_starts)):\n",
    "        # Rest periods\n",
    "        t_start = rest_starts[rest]\n",
    "        t_end = rest_ends[rest]\n",
    "        rest_epoch = EEGdata.loc[t_start:t_end][channels]\n",
    "        \n",
    "        # Use previous data to normalize (using -10 to -2 seconds to get pure MI action)\n",
    "        tb_start = rest_starts[rest] - np.round(10.00 * fs)\n",
    "        tb_end = rest_starts[rest] - np.round(2.00 * fs)\n",
    "        baseline = EEGdata.loc[tb_start:tb_end][channels]\n",
    "\n",
    "        # Store epoch\n",
    "        tmp = (rest_epoch - np.mean(baseline))/np.std(baseline)\n",
    "        epochs.append(rest_epoch)\n",
    "        epochs_norm.append(tmp)\n",
    "\n",
    "    return epochs, epochs_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables to Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the BCI data\n",
    "subjID = 'nile'\n",
    "EEGdevice = 8 # 7 for DSI-7, 8 for Enobio\n",
    "filename_eeg = 'SaveData/20190130122905_nile_BCI.easy'\n",
    "filename_behavioral = 'SaveData/BCI_nile_R1.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the BCI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping N/A\n",
      "Creating RawArray with float64 data, n_channels=8, n_times=138604\n",
      "    Range : 0 ... 138603 =      0.000 ...   277.206 secs\n",
      "Ready.\n",
      "Setting up band-pass filter from 1 - 40 Hz\n",
      "l_trans_bandwidth chosen to be 1.0 Hz\n",
      "h_trans_bandwidth chosen to be 10.0 Hz\n",
      "Filter length of 1651 samples (3.302 sec) selected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nilew\\AppData\\Local\\Continuum\\anaconda2\\envs\\py36\\lib\\site-packages\\scipy\\signal\\_arraytools.py:45: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  b = a[a_slice]\n",
      "C:\\Users\\nilew\\AppData\\Local\\Continuum\\anaconda2\\envs\\py36\\lib\\site-packages\\scipy\\signal\\signaltools.py:3463: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return y[sl]\n"
     ]
    }
   ],
   "source": [
    "# Load EEG data\n",
    "EEGdata, fs, fs_accel = LoadEEGData(filename_eeg, EEGdevice)\n",
    "\n",
    "# Load behavioral data\n",
    "behavioralData = LoadBehavioralDataBCI(filename_behavioral)\n",
    "\n",
    "# Sync up trigger pulses\n",
    "num_of_trials, num_of_movements, move_starts, hasBaseline, rest_starts, rest_ends = SyncTriggerPulsesBCI(EEGdata, EEGdevice, fs, behavioralData)\n",
    "\n",
    "# Clean the data\n",
    "EEGdata_orig = EEGdata.copy()\n",
    "lf = 1\n",
    "hf = 40\n",
    "\n",
    "if EEGdevice == 7:\n",
    "    channels = EEGdata.columns[1:8]\n",
    "elif EEGdevice == 8:\n",
    "    channels = EEGdata.columns[0:8]\n",
    "\n",
    "# Format our data into an mne-friendly format\n",
    "ch_types = ['eeg']*len(channels)\n",
    "info = mne.create_info(ch_names=list(channels), sfreq=fs, ch_types=ch_types)\n",
    "rawData = EEGdata[channels].values\n",
    "rawData = np.transpose(rawData)\n",
    "raw = mne.io.array.RawArray(rawData, info)\n",
    "raw.set_montage(mne.channels.read_montage(kind='standard_1020'))\n",
    "raw.filter(l_freq=lf, h_freq=hf)\n",
    "\n",
    "# Make a copy of the original data just in case\n",
    "EEGdata[channels] = raw.get_data().T\n",
    "\n",
    "# Epoch the data\n",
    "epochs, epochs_norm = EpochBCIData(EEGdata, move_starts, rest_starts, rest_ends)\n",
    "\n",
    "# Organize trial types\n",
    "trial_type = OrganizeTrials(behavioralData, hasBaseline)\n",
    "\n",
    "# Get signal features\n",
    "alpha_power, beta_power = ExtractFeaturesBCI(epochs_norm, num_of_movements, ['C3','C4'], 1)\n",
    "motor_features = [alpha_power['C3'], alpha_power['C4'], beta_power['C3'], beta_power['C4']]\n",
    "motor_features = np.transpose(motor_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the MI classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nilew\\AppData\\Local\\Continuum\\anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\base.py:251: UserWarning: Trying to unpickle estimator SVC from version 0.20.2 when using version 0.20.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Load latest model and its associated data\n",
    "models = glob.glob('Models/' + subjID + '_MI_classifier_*')\n",
    "model_file = models[-1] # load the most recent model\n",
    "MI_model = pickle.load(open(model_file, 'rb'))\n",
    "\n",
    "models_data_list = glob.glob('Models/' + subjID + '_data_for_MI_classifier_*')\n",
    "models_data = models_data_list[-1] # load the most recent model\n",
    "MI_data = np.load(models_data)\n",
    "X_loaded_MI = MI_data['X_not_scaled']\n",
    "y_loaded_MI = MI_data['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find ErrPs in current BCI data (this function loads the most recent ErrP model for this subject, epochs for error, creates features, and classifies) and also estimate attention.\n",
    "\n",
    "Make sure you've run Create_Error_Decoder first (right now it's just an .ipnyb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/nile\\nile_Error_classifier_2019-01-30-14-38-17.sav\n",
      "MLPClassifier(activation='tanh', alpha=0.1, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "       random_state=1, shuffle=True, solver='lbfgs', tol=0.0001,\n",
      "       validation_fraction=0.1, verbose=False, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nilew\\AppData\\Local\\Continuum\\anaconda2\\envs\\py36\\lib\\site-packages\\scipy\\signal\\_arraytools.py:45: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  b = a[a_slice]\n",
      "C:\\Users\\nilew\\AppData\\Local\\Continuum\\anaconda2\\envs\\py36\\lib\\site-packages\\scipy\\signal\\signaltools.py:3463: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return y[sl]\n",
      "C:\\Users\\nilew\\AppData\\Local\\Continuum\\anaconda2\\envs\\py36\\lib\\site-packages\\scipy\\signal\\_arraytools.py:45: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  b = a[a_slice]\n",
      "C:\\Users\\nilew\\AppData\\Local\\Continuum\\anaconda2\\envs\\py36\\lib\\site-packages\\scipy\\signal\\signaltools.py:3463: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return y[sl]\n"
     ]
    }
   ],
   "source": [
    "# Choose which examples to keep through confidence scoring\n",
    "X_new = motor_features\n",
    "y_new = trial_type\n",
    "\n",
    "CS, TL, preds_MI, preds_error, preds_error_proba, epochs, features, clf_error = ConfidenceScoreExamples(X_new, y_new, EEGdata, epochs_norm, EEGdevice, fs, num_of_movements, move_starts, trial_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of examples: 95\n",
      "Percent of correctly scored examples: 55.78947368421052%\n",
      "----------------\n",
      "Confusion Matrix for Error Classification (not MI)\n",
      "[[41 22]\n",
      " [20 12]]\n",
      "----------------\n"
     ]
    }
   ],
   "source": [
    "# clf_error: our ErrP classifier\n",
    "# TL: true labels for the performance monitoring epochs in this BCI data\n",
    "# features: the features for the ErrP classifier for these performance monitoring epochs\n",
    "\n",
    "print('Total number of examples: ' + str(len(features)))\n",
    "print('Percent of correctly scored examples: ' + str(clf_error.score(features, TL)*100) + '%')\n",
    "print('----------------')\n",
    "print('Confusion Matrix for Error Classification (not MI)')\n",
    "print(confusion_matrix(TL, clf_error.predict(features)))\n",
    "print('----------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrain / update\n",
    "Retrain and save MI classifier (with pre-scaled features), don't update error classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain\n",
    "adaptationType = 'CS' # either 'CS' for confidence score, or 'TL' for true label\n",
    "threshold = 0.7\n",
    "clf, X, X_not_scaled, y = RetrainDecoder(MI_model, CS, threshold, X_loaded_MI, y_loaded_MI, motor_features, trial_type, adaptationType)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check some values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "990"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_loaded_MI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.where(CS>0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1049"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save decoder and data it was trained/tested on\n",
    "SaveDecoderAndData(clf, X, X_not_scaled, y, subjID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
