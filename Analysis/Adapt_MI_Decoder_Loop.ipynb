{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adapt Motor Imagery classifier (loop) with confidence scores\n",
    "Use this clean notebook to create the code to update the MI classifier using confidence scores created from Error-related Potentials (ErrPs) and Default Mode Network (DMN) activity / attention. This notebook differs from `Adapt_MI_Decoder_Loop` in that it automatically creates all the decoders without you having to move files around (hide all files ahead of run of interest).\n",
    "\n",
    "Please see `Check_BCI_ErrP_with_Attention.ipnyb` for a version of this code with plotting / checks / tinkering\n",
    "\n",
    "Nile Wilson 2019.02.06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.fftpack import fft, ifft\n",
    "from scipy import signal\n",
    "from mne.filter import filter_data\n",
    "\n",
    "import scipy.signal as scisig\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import pickle\n",
    "import glob\n",
    "import csv\n",
    "import mne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables to Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the BCI data\n",
    "subjID = 'b12a46'\n",
    "subject_group = 1\n",
    "session_number = 3\n",
    "EEGdevice = 8 # 7 for DSI-7, 8 for Enobio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load latest model and its associated data\n",
    "BCI_files = glob.glob('SaveData/' + subjID + '/Session' + str(session_number) + '/*' + subjID + '_BCI.easy')\n",
    "num_of_runs = len(BCI_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected number of BCI runs: 0\n"
     ]
    }
   ],
   "source": [
    "# Check that this is correct\n",
    "print('Detected number of BCI runs: ' + str(num_of_runs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for working with the BCI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindBCIFiles(subjID, session_number, run_number):\n",
    "    \"\"\"\n",
    "    Returns the file names for the BCI run of choice\n",
    "    \"\"\"\n",
    "    # Load latest model and its associated data\n",
    "    BCI_files = glob.glob('SaveData/' + subjID + '/Session' + str(session_number) + '/*' + subjID + '_BCI.easy')\n",
    "    \n",
    "    # Throw exception if model number is outside range of existing models\n",
    "    if run_number-1 > len(BCI_files):\n",
    "        raise ValueError('Please select a valid run number')\n",
    "    \n",
    "    filename_eeg = BCI_files[run_number-1]\n",
    "    print('Selecting BCI EEG file: ' + filename_eeg)\n",
    "\n",
    "    behavioral_files = glob.glob('SaveData/' + subjID + '/Session' + str(session_number) + '/BCI_' + subjID + '*.csv')\n",
    "    filename_behavioral = behavioral_files[run_number-1]\n",
    "    print('Selecting BCI behavioral file: ' + filename_behavioral)\n",
    "    \n",
    "    return filename_eeg, filename_behavioral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadEEGData(filename, EEGdevice):\n",
    "    \"\"\" This function converts a single .easy file (from NIC2) to an easy-to-use dataframe.\n",
    "    Uses both the .easy file and .info file (containing metadata)\n",
    "    \n",
    "    ---- Input ----\n",
    "    filename: string containing the .easy filepath\n",
    "    \n",
    "    ---- Output ----\n",
    "    df: dataframe containing all the EEG, accelerometer, and event marker data\n",
    "    fs: sampling rate for the EEG data (Hz)\n",
    "    fs_accel: sampling rate for the accelerometer data (Hz)\n",
    "    \n",
    "    \"\"\"\n",
    "    if EEGdevice == 7:\n",
    "        x = 1\n",
    "    elif EEGdevice == 8:\n",
    "        # Read in the .easy file\n",
    "        df = pd.read_csv(filename, delimiter='\\t', header=None)\n",
    "\n",
    "        # Get metadata from the .info file\n",
    "        fname = filename[:-5] + '.info'\n",
    "        with open(fname) as f:\n",
    "            content = f.readlines()\n",
    "        content = [x.strip() for x in content]\n",
    "\n",
    "        # Get the channel names\n",
    "        channel_info = [x for x in content if 'Channel ' in x]\n",
    "        channel_names = []\n",
    "        for ch in range(len(channel_info)):\n",
    "            channel_names.append(channel_info[ch].split(': ')[1])\n",
    "\n",
    "        channel_names.append('X')\n",
    "        channel_names.append('Y')\n",
    "        channel_names.append('Z')\n",
    "        channel_names.append('STI 014')\n",
    "        channel_names.append('DateTime')\n",
    "\n",
    "        # Get sampling rates\n",
    "        sampling_rates = [x for x in content if 'sampling rate: ' in x]\n",
    "        fs_all = []\n",
    "        for freq in range(len(sampling_rates)):\n",
    "            tmp = sampling_rates[freq].split(': ')[1].split(' ')[0]\n",
    "            if tmp in ['N/A']:\n",
    "                print('Skipping N/A')\n",
    "            else:\n",
    "                fs_all.append(float(sampling_rates[freq].split(': ')[1].split(' ')[0]))\n",
    "\n",
    "        # Store sampling rates\n",
    "        fs = fs_all[0]\n",
    "        fs_accel = fs_all[1]\n",
    "\n",
    "        # Assign the column names\n",
    "        df.columns = channel_names\n",
    "    \n",
    "    # Return dataframe and sampling rates\n",
    "    return df, fs, fs_accel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadBehavioralDataBCI(filename_behavioral):\n",
    "    \"\"\"\n",
    "    This function loads behavioral data for the motor screening task and formats it to use in this script\n",
    "    \"\"\"\n",
    "    behavioralData = pd.read_csv(filename_behavioral, ',')\n",
    "    \n",
    "    return behavioralData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SyncTriggerPulsesBCI(EEGdata, EEGdevice, fs, behavioralData):\n",
    "    \"\"\"\n",
    "    This function returns the indices for events of interest\n",
    "    \"\"\"\n",
    "    \n",
    "    if EEGdevice == 7:\n",
    "        print('Put code here')\n",
    "    elif EEGdevice == 8:\n",
    "        # Store where the values in trigger are equal to 8 (the audio trigger input channel number)\n",
    "        index_trigger = np.where(EEGdata['STI 014']!=0)\n",
    "        index_trigger = index_trigger[0]\n",
    "\n",
    "        # Number of trials is greater than number of total pulses sent\n",
    "        # 999 when the task ends\n",
    "        move_left_starts = np.where(EEGdata['STI 014'] == 1)[0]\n",
    "        move_right_starts = np.where(EEGdata['STI 014'] == 2)[0]\n",
    "        rest_starts = np.where(EEGdata['STI 014'] == 3)[0]\n",
    "        rest_ends = np.where(EEGdata['STI 014'] == 4)[0]\n",
    "        \n",
    "        # If the number of rest_starts and rest_ends don't match, drop the extra one\n",
    "        # there should, by default, only be 12 starts and 12 ends\n",
    "\n",
    "        if len(rest_ends) > len(rest_starts):\n",
    "            if rest_ends[0] < rest_starts[0]:\n",
    "                rest_ends = rest_ends[1:]\n",
    "        elif len(rest_ends) < len(rest_starts):\n",
    "            if rest_ends[0] > rest_starts[0]:\n",
    "                rest_starts = rest_starts[1:]\n",
    "        \n",
    "        move_starts = np.sort(np.concatenate((move_left_starts,move_right_starts),0))\n",
    "        total_movements = len(move_starts)\n",
    "\n",
    "        # exclude movements that occur without defined baseline (if you need to get rid of first rest)\n",
    "        hasBaseline = list()\n",
    "        for movement in range(0,len(move_starts)):\n",
    "            hasBaseline.append(True in (rest_starts < move_starts[movement]))\n",
    "\n",
    "        np.where(hasBaseline)\n",
    "        move_starts = move_starts[np.where(hasBaseline)]\n",
    "\n",
    "        # exclude the move lefts and move rights that were thrown out in move_starts\n",
    "        for movement in range(0,total_movements):\n",
    "            if hasBaseline[movement] is False:\n",
    "                # for the left movements\n",
    "                idx_left = np.where(move_left_starts == move_starts[movement])\n",
    "                idx_left = np.asarray(idx_left)\n",
    "                idx_right = np.where(move_right_starts == move_starts[movement])\n",
    "                idx_right = np.asarray(idx_right)\n",
    "\n",
    "                if idx_left.size > 0:\n",
    "                    move_left_starts = np.delete(move_left_starts, idx_left)\n",
    "                if idx_right.size > 0:\n",
    "                    move_right_starts = np.delete(move_right_starts, idx_right)\n",
    "                \n",
    "        num_of_trials = len(rest_starts)\n",
    "        num_of_movements = len(move_left_starts) + len(move_right_starts)\n",
    "    \n",
    "    return num_of_trials, num_of_movements, move_starts, hasBaseline, rest_starts, rest_ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EpochBCIData(EEGdata, fs, move_starts, rest_starts, rest_ends):\n",
    "    \"\"\"\n",
    "    This function epochs the data\n",
    "    \"\"\"\n",
    "    \n",
    "    if EEGdevice == 7:\n",
    "        channels = EEGdata.columns[1:8]\n",
    "    elif EEGdevice == 8:\n",
    "        channels = EEGdata.columns[0:8]\n",
    "\n",
    "    epochs = []\n",
    "    epochs_norm = []\n",
    "\n",
    "    for movement in range(0,len(move_starts)):\n",
    "        # Data for this movement\n",
    "        t_start = move_starts[movement] - np.round(2.00*fs)\n",
    "        t_end = move_starts[movement] - np.round(0.250*fs)\n",
    "\n",
    "        # Baseline\n",
    "        restOfInt = np.max(np.where(rest_starts < move_starts[movement]))\n",
    "        tb_start = rest_starts[restOfInt]\n",
    "        tb_end = rest_ends[restOfInt]\n",
    "\n",
    "        # Note, baseline is filtered before it is used in the live script\n",
    "        baseline_pre_filt = np.asarray(EEGdata.loc[tb_start:tb_end][channels]) * 1.0\n",
    "        baseline_filt = filter_data(baseline_pre_filt.T, sfreq=fs, l_freq=7, h_freq=31, verbose='ERROR')\n",
    "        baseline = baseline_filt.T\n",
    "\n",
    "        # Filter per epoch, like you would in real-time\n",
    "        pre_filt = np.asarray(EEGdata.loc[t_start:t_end][channels]) * 1.0\n",
    "        filtered = filter_data(pre_filt.T, sfreq=fs, l_freq=7, h_freq=31, verbose='ERROR')\n",
    "        epoch = pd.DataFrame(filtered.T)\n",
    "        epoch.columns = EEGdata.columns[0:8]\n",
    "\n",
    "        # Store epoch\n",
    "        tmp = (epoch - np.mean(baseline))/np.std(baseline)\n",
    "        tmp = pd.DataFrame(tmp)\n",
    "        tmp.columns = EEGdata.columns[0:8]\n",
    "\n",
    "        epochs_norm.append(tmp)\n",
    "        epochs.append(epoch)\n",
    "\n",
    "    return epochs, epochs_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OrganizeTrials(behavioralData, hasBaseline):\n",
    "    \"\"\"\n",
    "    Organizes trials\n",
    "    \"\"\"\n",
    "    \n",
    "    # When target was to the left\n",
    "    trialL = np.where(behavioralData['target_x'] < 1000)\n",
    "    \n",
    "    # When target was to the right\n",
    "    trialR = np.where(behavioralData['target_x'] > 1000)\n",
    "    \n",
    "    # Create a single list that includes which trial is which (L = 0, R = 1)\n",
    "    trial_type = np.zeros([1,len(behavioralData['score'])])\n",
    "    trial_type[0][trialL] = 0\n",
    "    trial_type[0][trialR] = 1\n",
    "\n",
    "    trial_type = np.round(trial_type[0])\n",
    "    \n",
    "    # Remove trials if no baseline\n",
    "    for movement in range(0,len(hasBaseline)):\n",
    "        if hasBaseline[movement] is False:\n",
    "            trial_type = np.delete(trial_type, movement)\n",
    "            \n",
    "    return trial_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractFeaturesBCI(epochs, num_of_movements, channelsToUse, ds_factor):\n",
    "    \"\"\"\n",
    "    Extract signal features of interest\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the summed delta power for each trial\n",
    "    alpha_power = dict.fromkeys(channelsToUse)\n",
    "    beta_power = dict.fromkeys(channelsToUse)\n",
    "    ds_f = ds_factor # downsampling factor\n",
    "\n",
    "    for chanOfInt in channelsToUse:\n",
    "        tmp_alpha = list()\n",
    "        tmp_beta = list()\n",
    "\n",
    "        for movement in range(0, num_of_movements):\n",
    "            f, Pxx_den = signal.welch(signal.decimate(epochs[movement][chanOfInt],ds_f), fs/ds_f, scaling='spectrum')\n",
    "            alpha_idx = np.where(np.logical_and(np.round(f) > 8, np.round(f) <= 12))\n",
    "            tmp_alpha.append(np.sum(Pxx_den[alpha_idx]))\n",
    "\n",
    "            beta_idx = np.where(np.logical_and(np.round(f) > 13, np.round(f) <= 30))\n",
    "            tmp_beta.append(np.sum(Pxx_den[beta_idx]))\n",
    "\n",
    "        alpha_power[chanOfInt] = tmp_alpha\n",
    "        beta_power[chanOfInt] = tmp_beta\n",
    "    \n",
    "    return alpha_power, beta_power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for working with error (ErrPs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EpochErrorData(EEGdata, fs, EEGdevice, t_trial_start):\n",
    "    \"\"\"\n",
    "    This function epochs the data\n",
    "    \"\"\"\n",
    "    if EEGdevice == 7:\n",
    "        channels = EEGdata.columns[1:8]\n",
    "    elif EEGdevice == 8:\n",
    "        channels = EEGdata.columns[0:8]\n",
    "\n",
    "    epochs = []\n",
    "\n",
    "    for trial in range(0,len(t_trial_start)):\n",
    "        t_start = t_trial_start[trial] - np.round(0  * fs)\n",
    "        t_end = t_trial_start[trial] + np.round(0.600 * fs)\n",
    "\n",
    "        # Baseline\n",
    "        tb_start = t_trial_start[trial] - np.round(0.700 * fs)\n",
    "        tb_end = t_trial_start[trial] - np.round(0.100 * fs)\n",
    "        baseline = EEGdata.loc[tb_start:tb_end][channels]\n",
    "\n",
    "        # Store epoch\n",
    "        tmp = (EEGdata.loc[t_start:t_end][channels] - np.mean(baseline))/np.std(baseline)\n",
    "        epochs.append(tmp)\n",
    "    \n",
    "    return epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractErrorFeatures(epochs, num_of_trials, error_template, correct_template, featureType):\n",
    "    \"\"\"\n",
    "    Extract signal features of interest\n",
    "    featureType:    'template' or 'frequency'. 'template' returns features based on the template projection values\n",
    "                    for individual epochs with the error and correct templates. 'frequency' returns features that\n",
    "                    are just delta and theta power for each channel for the epochs\n",
    "    \"\"\"\n",
    "    \n",
    "    if featureType in ['template','Template','TEMPLATE','t','T','projection','Projection','PROJECTION','p','P']:\n",
    "        # template_projection[chanOfInt] will have two columns\n",
    "        # col 1: how well the trial signal matches with the error signal template\n",
    "        # col 2: how well the trial signal matches with the correct signal template\n",
    "        projections_all = dict()\n",
    "        channelsToUse = error_template.keys()\n",
    "\n",
    "        for chanOfInt in channelsToUse:\n",
    "            projections = np.zeros([2, num_of_trials])\n",
    "            for trial in range(0, num_of_trials):\n",
    "                # Individual epoch (normalized)\n",
    "                tmp = epochs_norm[trial][chanOfInt]\n",
    "                a = tmp\n",
    "\n",
    "                # Template waveform for error (normalized)\n",
    "                tmp0 = error_template[chanOfInt]\n",
    "                tmp_norm = (tmp0 - np.mean(tmp0))/np.std(tmp0)\n",
    "                b = tmp_norm\n",
    "\n",
    "                # Template waveform for correct (normalized)\n",
    "                tmp = correct_template[chanOfInt]\n",
    "                tmp_norm = (tmp - np.mean(tmp0))/np.std(tmp0)\n",
    "                c = tmp_norm\n",
    "\n",
    "                # Store sum of convolutions\n",
    "\n",
    "                projections[0][trial] = np.sum(np.convolve(a,b,'same'))\n",
    "                projections[1][trial] = np.sum(np.convolve(a,c,'same'))\n",
    "\n",
    "            projections_all[chanOfInt] = projections\n",
    "        \n",
    "        # Organize the features\n",
    "        channels = list(projections_all.keys())\n",
    "        num_of_features = np.shape(projections_all['Cz'])[0] * len(channels)\n",
    "        channels_full = list(projections_all.keys()) * 2\n",
    "        num_of_trials = np.shape(projections_all['Cz'])[1]\n",
    "\n",
    "        features = np.zeros([num_of_features, num_of_trials])\n",
    "\n",
    "        for trial in range(0, num_of_trials):\n",
    "            # Error trials are 0 to num_of_features//2, and correct trials are num_of_features//2 to num_of_features\n",
    "            for feature in range(0, num_of_features):\n",
    "                features[feature, trial] = projections_all[channels_full[feature]][0][trial]\n",
    "            \n",
    "    elif featureType in ['frequency','Frequency','FREQUENCY','f','F']:\n",
    "        channelsToUse = error_template.keys()\n",
    "        delta_power = dict.fromkeys(channelsToUse)\n",
    "        theta_power = dict.fromkeys(channelsToUse)\n",
    "        ds_f = 1 # downsampling factor\n",
    "\n",
    "        for chanOfInt in channelsToUse:\n",
    "            tmp_delta = list()\n",
    "            tmp_theta = list()\n",
    "\n",
    "            for trial in range(0, num_of_trials):\n",
    "                f, Pxx_den = signal.welch(signal.decimate(epochs_norm[trial][chanOfInt],ds_f), fs/ds_f, scaling='spectrum')\n",
    "                delta_idx = np.where(np.round(f) <= 4)\n",
    "                tmp_delta.append(np.sum(Pxx_den[delta_idx]))\n",
    "\n",
    "                theta_idx = np.where(np.logical_and(np.round(f) > 4, np.round(f) <= 7))\n",
    "                tmp_theta.append(np.sum(Pxx_den[theta_idx]))\n",
    "\n",
    "            delta_power[chanOfInt] = tmp_delta\n",
    "            theta_power[chanOfInt] = tmp_theta\n",
    "            \n",
    "        # Organize the features\n",
    "        num_of_examples = len(delta_power['Cz'])\n",
    "        num_of_features = len(delta_power.keys()) + len(theta_power.keys()) \n",
    "        features = np.zeros([num_of_features, num_of_examples])\n",
    "\n",
    "        # Get all channels in one list to loop through\n",
    "        feature_channels = np.concatenate([np.asarray(list(delta_power.keys())),np.asarray(list(theta_power.keys()))])\n",
    "\n",
    "        for i in range(0, num_of_examples):\n",
    "            for j in range(0, num_of_features//2):\n",
    "                features[j, i] = delta_power[feature_channels[j]][i]\n",
    "            for j in range(num_of_features//2, num_of_features):\n",
    "                features[j, i] = theta_power[feature_channels[j]][i]\n",
    "\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConfidenceScoreExamples(X, y, EEGdata, epochs_norm, EEGdevice, fs, num_of_movements, move_starts, trial_type):\n",
    "    \"\"\"\n",
    "    This is the function that does the confidence scoring based on error detection and attention\n",
    "    \"\"\"\n",
    "    # Load the error detection model and see what featureType it used (frequency or template projections)\n",
    "    models = glob.glob('Models/' + subjID + '/Session1/' + subjID + '_Error_classifier_*') #always load session1 error\n",
    "    model_file = models[-1] # load the most recent model\n",
    "    clf_error = pickle.load(open(model_file, 'rb'))\n",
    "    print(model_file)\n",
    "    print(clf_error)\n",
    "\n",
    "    models_data_list = glob.glob('Models/' + subjID + '/Session1/' + subjID + '_data_for_Error_classifier_*')\n",
    "    models_data = models_data_list[-1] # load the most recent model\n",
    "    loaded_data = np.load(models_data)\n",
    "    featureType = loaded_data['featureType']\n",
    "    \n",
    "    # Load templates if applicable\n",
    "    error_template = loaded_data['error_template'].tolist()\n",
    "    correct_template = loaded_data['correct_template'].tolist()\n",
    "\n",
    "    # Create new epochs for error detection\n",
    "    epochs = EpochErrorData(EEGdata, fs, EEGdevice, move_starts)\n",
    "    features = ExtractErrorFeatures(epochs, num_of_movements, error_template, correct_template, featureType)\n",
    "    features = features.T\n",
    "    \n",
    "    # Scale the features\n",
    "    features = StandardScaler().fit_transform(features)\n",
    "    \n",
    "    # Detect error\n",
    "    preds_error = clf_error.predict(features) # is there an ErrP or not? 1 = yes ErrP, 0 = no ErrP\n",
    "    preds_error_proba = clf_error.predict_proba(features) # what is the prob of there being an ErrP?\n",
    "    \n",
    "    # Confidence in the prediction of error\n",
    "    prob_error = (preds_error_proba[:,1] * preds_error)\n",
    "\n",
    "    # Confidence in the prediction of no error\n",
    "    prob_no_error = preds_error_proba[:,0] * (1-preds_error)\n",
    "    \n",
    "    # Get values for attention\n",
    "    attention_alpha, attention_beta = ExtractFeaturesBCI(epochs_norm, num_of_movements, ['Pz'], 1)\n",
    "    epochs_rest, epochs_rest_norm = GetRestEpochsBCI(EEGdata, rest_starts, rest_ends)\n",
    "    attention_alpha_rest_norm, attention_beta_rest_norm = ExtractFeaturesBCI(epochs_rest_norm, len(epochs_rest), ['Pz'], 1)\n",
    "    beta_threshold = np.mean(attention_beta_rest_norm['Pz'])-(2*np.std(attention_beta_rest_norm['Pz']))\n",
    "    distance_from_threshold = (attention_beta['Pz']-beta_threshold)/np.max(attention_beta_rest_norm['Pz'])\n",
    "\n",
    "    # Confidence the epoch is correct\n",
    "    w_a = 0.5\n",
    "    CS_pre_scale = ((prob_no_error - prob_error + 1) - (w_a*distance_from_threshold))/2\n",
    "    CS = (CS_pre_scale - np.min(CS_pre_scale))/(np.max(CS_pre_scale)-np.min(CS_pre_scale))\n",
    "    \n",
    "    \"\"\"\n",
    "    Also return true label stuff\n",
    "    \"\"\"\n",
    "    # Also return true label scores\n",
    "    # Load the error detection model and see what featureType it used (frequency or template projections)\n",
    "    models = glob.glob('Models/' + subjID + '/Session1/' + subjID + '_MI_classifier_*')\n",
    "    model_file = models[-1] # load the most recent model\n",
    "    clf_MI = pickle.load(open(model_file, 'rb'))\n",
    "\n",
    "    preds_MI = clf_MI.predict(X)\n",
    "    \n",
    "    # trial_type 0 is L, trial_type 1 is R, and TL 1 is high confidence (correct), and TL 0 is low confidence (error)\n",
    "    TL = list()\n",
    "    for trial in range(0, len(trial_type)):\n",
    "        if trial_type[trial] == 0:\n",
    "            if preds_MI[trial] == 0:\n",
    "                TL.append(1)\n",
    "            else:\n",
    "                TL.append(0)\n",
    "        elif trial_type[trial] == 1:\n",
    "            if preds_MI[trial] == 1:\n",
    "                TL.append(1)\n",
    "            else:\n",
    "                TL.append(0)\n",
    "\n",
    "    return CS, TL, preds_MI, preds_error, preds_error_proba, epochs, features, clf_error, beta_threshold, attention_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RetrainDecoder(clf, CS, threshold, X_old, y_old, X_new, y_new, adaptationType):\n",
    "    \"\"\"\n",
    "    Retrains the decoder on ALL the previous data plus what you decided to add in with high confidence scores\n",
    "    \n",
    "    clf: your preloaded model (most recent version)\n",
    "    CS: confidence scores (if true labels, use TL instead of CS)\n",
    "    X_old: your preloaded model data (X)\n",
    "    y_old: your preloaded model data (y)\n",
    "    X_new: your new motor features\n",
    "    y_new: your new labels\n",
    "    \"\"\"\n",
    "    # Concatenate old X and y with new X and y that have a high enough confidence score\n",
    "    aboveThreshold = np.where(CS>threshold)\n",
    "    \n",
    "    X = np.concatenate((X_old, X_new[aboveThreshold]), axis=0)\n",
    "    y = np.concatenate((y_old, np.asarray(y_new)[aboveThreshold]), axis=0)\n",
    "    \n",
    "    # preprocess dataset, split into training and test part\n",
    "    args = np.arange(len(X))\n",
    "    np.random.shuffle(args)\n",
    "    X = [X[i] for i in args]\n",
    "    y = [y[i] for i in args]\n",
    "    X_not_scaled = X\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    \n",
    "    # Resample to account for imbalance introduced by CS thresholding\n",
    "    method = SMOTE(kind='regular')\n",
    "    X_balanced, y_balanced = method.fit_sample(X, y)\n",
    "\n",
    "    # Fit the model\n",
    "    clf.fit(X_balanced,y_balanced)\n",
    "    \n",
    "    return clf, X, X_not_scaled, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SaveDecoderAndData(clf, X, X_not_scaled, y, subjID, session_number, adaptationType):\n",
    "    \"\"\"\n",
    "    Save the decoder and the data it was trained/tested on\n",
    "    \"\"\"\n",
    "    time_to_save = datetime.datetime.now().isoformat()\n",
    "    time_to_save = time_to_save.replace('T','-')\n",
    "    time_to_save = time_to_save.replace(':','-')\n",
    "    \n",
    "    model = clf\n",
    "    if adaptationType == 'CS':\n",
    "        model_file = 'Models/' + subjID + '/Session' + str(session_number) + '/' + subjID + '_MI_classifier_' + time_to_save[:19] + '.sav'\n",
    "        filepath_export_data = 'Models/' + subjID + '/Session' + str(session_number) + '/' + subjID + '_data_for_MI_classifier_' + time_to_save[:19] + '.npz'\n",
    "    elif adaptationType == 'TL':\n",
    "        model_file = 'Models/' + subjID + '/Session' + str(session_number) + '/' + subjID + '_MI_classifier_TL_' + time_to_save[:19] + '.sav'\n",
    "        filepath_export_data = 'Models/' + subjID + '/Session' + str(session_number) + '/' + subjID + '_data_for_MI_classifier_TL_' + time_to_save[:19] + '.npz'\n",
    "\n",
    "    pickle.dump(model, open(model_file, 'wb'))\n",
    "    np.savez_compressed(filepath_export_data, X=X, X_not_scaled=X_not_scaled, y=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to work with attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetRestEpochsBCI(EEGdata, rest_starts, rest_ends):\n",
    "    \"\"\"\n",
    "    This function returns the rest (ITI) epochs that were used for baseline normalization in the BCI\n",
    "    \"\"\" \n",
    "    if EEGdevice == 7:\n",
    "        channels = EEGdata.columns[1:8]\n",
    "    elif EEGdevice == 8:\n",
    "        channels = EEGdata.columns[0:8]\n",
    "\n",
    "    epochs = []\n",
    "    epochs_norm = []\n",
    "\n",
    "    for rest in range(0,len(rest_starts)):\n",
    "        # Rest periods\n",
    "        t_start = rest_starts[rest]\n",
    "        t_end = rest_ends[rest]\n",
    "        rest_epoch = EEGdata.loc[t_start:t_end][channels]\n",
    "        \n",
    "        # Use previous data to normalize (using -10 to -2 seconds to get pure MI action)\n",
    "        tb_start = rest_starts[rest] - np.round(10.00 * fs)\n",
    "        tb_end = rest_starts[rest] - np.round(2.00 * fs)\n",
    "        baseline = EEGdata.loc[tb_start:tb_end][channels]\n",
    "\n",
    "        # Store epoch\n",
    "        tmp = (rest_epoch - np.mean(baseline))/np.std(baseline)\n",
    "        epochs.append(rest_epoch)\n",
    "        epochs_norm.append(tmp)\n",
    "\n",
    "    return epochs, epochs_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadModel_MI(subjID, run_number=0, session_number=1, adaptationType='CS'):\n",
    "    \"\"\"\n",
    "    This function loads the most recent motor imagery classifier\n",
    "    for this subject by default. Current run minus 1.\n",
    "    \n",
    "    You may also select which number model (from original to most recent)\n",
    "    by using a value from 0 (original) to 4 (most recent)\n",
    "    \"\"\"\n",
    "    runOfInt= run_number\n",
    "    run_number = runOfInt - 1\n",
    "    # Load latest model and its associated data\n",
    "    if run_number == 1:\n",
    "        models = glob.glob('Models/' + subjID + '/Session' + str(session_number) + '/' + subjID + '_MI_classifier_*')\n",
    "        models_data_list = glob.glob('Models/' + subjID + '/Session' + str(session_number) + '/' + subjID + '_data_for_MI_classifier_*')\n",
    "    else:\n",
    "        if adaptationType == 'CS' or adaptationType == 'None':\n",
    "            models = glob.glob('Models/' + subjID + '/Session' + str(session_number) + '/' + subjID + '_MI_classifier_*')\n",
    "            models_data_list = glob.glob('Models/' + subjID + '/Session' + str(session_number) + '/' + subjID + '_data_for_MI_classifier_*')\n",
    "        elif adaptationType == 'TL':\n",
    "            models = glob.glob('Models/' + subjID + '/Session' + str(session_number) + '/' + subjID + '_MI_classifier_TL_*')\n",
    "            models_data_list = glob.glob('Models/' + subjID + '/Session' + str(session_number) + '/' + subjID + '_data_for_MI_classifier_TL_*')\n",
    "    \n",
    "    # Throw exception if model number is outside range of existing models\n",
    "    if run_number > len(models):\n",
    "        raise ValueError('Please select a valid model number')\n",
    "    if adaptationType == 'TL':\n",
    "        run_number = -1\n",
    "    print('run_number: ' + str(run_number))\n",
    "    model_file = models[run_number] # load the most recent model\n",
    "    print('Selecting model: ' + model_file)\n",
    "    clf = pickle.load(open(model_file, 'rb'))\n",
    "\n",
    "    models_data = models_data_list[run_number] # load the most recent model\n",
    "    MI_data = np.load(models_data)\n",
    "    X_loaded = MI_data['X_not_scaled']\n",
    "    y_loaded = MI_data['y']\n",
    "    \n",
    "    return clf, X_loaded, y_loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop it to run automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run adaptation for all runs except the last one\n",
    "error_scores = dict()\n",
    "CS_all = dict()\n",
    "TL_all = dict()\n",
    "clf_error_all = dict()\n",
    "error_features_all = dict()\n",
    "beta_threshold_all = dict()\n",
    "attention_beta_all = dict()\n",
    "\n",
    "for runOfInt in range(1, num_of_runs+1):\n",
    "    # Find BCI file names\n",
    "    filename_eeg, filename_behavioral = FindBCIFiles(subjID, session_number, runOfInt)\n",
    "\n",
    "    # Load EEG data\n",
    "    EEGdata, fs, fs_accel = LoadEEGData(filename_eeg, EEGdevice)\n",
    "\n",
    "    # Load behavioral data\n",
    "    behavioralData = LoadBehavioralDataBCI(filename_behavioral)\n",
    "\n",
    "    # Sync up trigger pulses\n",
    "    num_of_trials, num_of_movements, move_starts, hasBaseline, rest_starts, rest_ends = SyncTriggerPulsesBCI(EEGdata, EEGdevice, fs, behavioralData)\n",
    "\n",
    "    # Epoch the data\n",
    "    epochs, epochs_norm = EpochBCIData(EEGdata, fs, move_starts, rest_starts, rest_ends)\n",
    "\n",
    "    # Organize trial types\n",
    "    trial_type = OrganizeTrials(behavioralData, hasBaseline)\n",
    "\n",
    "    # Get signal features\n",
    "    alpha_power, beta_power = ExtractFeaturesBCI(epochs_norm, num_of_movements, ['C3','C4'], 1)\n",
    "    motor_features = [alpha_power['C3'], alpha_power['C4'], beta_power['C3'], beta_power['C4']]\n",
    "    motor_features = np.transpose(motor_features)\n",
    "\n",
    "    # Load latest model and its associated data\n",
    "    if subject_group == 1:\n",
    "        adaptationType = 'None'\n",
    "    elif subject_group == 2:\n",
    "        adaptationType = 'CS'\n",
    "    MI_model, X_loaded_MI, y_loaded_MI = LoadModel_MI(subjID, runOfInt, session_number, adaptationType)\n",
    "\n",
    "    # Choose which examples to keep through confidence scoring\n",
    "    X_new = motor_features\n",
    "    y_new = trial_type\n",
    "\n",
    "    CS, TL, preds_MI, preds_error, preds_error_proba, epochs, features, clf_error, beta_threshold, attention_beta = ConfidenceScoreExamples(X_new, y_new, EEGdata, epochs_norm, EEGdevice, fs, num_of_movements, move_starts, trial_type)\n",
    "\n",
    "    # clf_error: our ErrP classifier\n",
    "    # TL: true labels for the performance monitoring epochs in this BCI data\n",
    "    # features: the features for the ErrP classifier for these performance monitoring epochs\n",
    "    error_scores[runOfInt] = clf_error.score(features, TL)\n",
    "    clf_error_all[runOfInt] = clf_error\n",
    "    CS_all[runOfInt] = CS\n",
    "    TL_all[runOfInt] = TL\n",
    "    error_features_all[runOfInt] = features\n",
    "    beta_threshold_all[runOfInt] = beta_threshold\n",
    "    attention_beta_all[runOfInt] = attention_beta\n",
    "\n",
    "    # Retrain on confidence scores\n",
    "    adaptationType = 'CS' # either 'CS' for confidence score, or 'TL' for true label\n",
    "    threshold = 0.7\n",
    "    clf, X, X_not_scaled, y = RetrainDecoder(MI_model, CS, threshold, X_loaded_MI, y_loaded_MI, motor_features, trial_type, adaptationType)\n",
    "\n",
    "    # Save decoder and data it was trained/tested on\n",
    "    if subject_group == 1:\n",
    "        if runOfInt < num_of_runs:\n",
    "            SaveDecoderAndData(clf, X, X_not_scaled, y, subjID, session_number, adaptationType)\n",
    "    else:\n",
    "        print('Not saving model (already created during BCI session)')\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Now adapt on true labels (load latest true label adaptation)\n",
    "    \"\"\"\n",
    "    # Retrain on true labels\n",
    "    adaptationType = 'TL' # either 'CS' for confidence score, or 'TL' for true label\n",
    "    if runOfInt > 1:\n",
    "        MI_model, X_loaded_MI, y_loaded_MI = LoadModel_MI(subjID, runOfInt, session_number, 'None')\n",
    "\n",
    "        # Choose which examples to keep through confidence scoring\n",
    "        X_new = motor_features\n",
    "        y_new = trial_type\n",
    "\n",
    "        CS, TL, preds_MI, preds_error, preds_error_proba, epochs, features, clf_error, beta_threshold, attention_beta = ConfidenceScoreExamples(X_new, y_new, EEGdata, epochs_norm, EEGdevice, fs, num_of_movements, move_starts, trial_type)\n",
    "\n",
    "    threshold = 0.7\n",
    "    clf, X, X_not_scaled, y = RetrainDecoder(MI_model, np.asarray(TL), threshold, X_loaded_MI, y_loaded_MI, motor_features, trial_type, adaptationType)\n",
    "    if runOfInt < num_of_runs:\n",
    "        SaveDecoderAndData(clf, X, X_not_scaled, y, subjID, session_number, adaptationType)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at confidence score creation\n",
    "Use this section of the notebook to explore the values that were used to create the confidence scores (CS), and compare the confidence scores against the true labels (TL).\n",
    "\n",
    "Plots are output to the Figures directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-6087e644b8b3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrunOfInt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Run '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrunOfInt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Total number of examples: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_features_all\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrunOfInt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     print('Percent of correctly scored examples: ' + str(clf_error_all[runOfInt].score(\n\u001b[0;32m      5\u001b[0m         error_features_all[runOfInt], TL_all[runOfInt])*100) + '%')\n",
      "\u001b[1;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "for runOfInt in range(1,5):\n",
    "    print('Run ' + str(runOfInt))\n",
    "    print('Total number of examples: ' + str(len(error_features_all[runOfInt])))\n",
    "    print('Percent of correctly scored examples: ' + str(clf_error_all[runOfInt].score(\n",
    "        error_features_all[runOfInt], TL_all[runOfInt])*100) + '%')\n",
    "    print('----------------')\n",
    "    print('Confusion Matrix for Error Classification (not MI)')\n",
    "    print(confusion_matrix(TL_all[runOfInt], clf_error_all[runOfInt].predict(error_features_all[runOfInt])))\n",
    "    print('----------------')\n",
    "\n",
    "    # Generate Plots\n",
    "    fig = plt.figure(figsize=(7,6))\n",
    "\n",
    "    # Plot threshold\n",
    "    plt.hlines(y=beta_threshold_all[runOfInt], xmin=0, xmax=len(attention_beta_all[runOfInt]['Pz']), linewidth=1, color='gray', linestyles='dashed')\n",
    "\n",
    "    # Plot different color for those above threshold\n",
    "    less_than_threshold = [i for i, val in enumerate(attention_beta_all[runOfInt]['Pz']) if val<=beta_threshold_all[runOfInt]]\n",
    "    greater_than_threshold = [i for i, val in enumerate(attention_beta_all[runOfInt]['Pz']) if val>beta_threshold_all[runOfInt]]\n",
    "\n",
    "    plt.plot(less_than_threshold, np.asarray(attention_beta_all[runOfInt]['Pz'])[less_than_threshold], 'ko', alpha=0.25)\n",
    "    plt.plot(greater_than_threshold, np.asarray(attention_beta_all[runOfInt]['Pz'])[greater_than_threshold], 'ko')\n",
    "\n",
    "    plt.xlabel('Examples')\n",
    "    plt.ylabel('Spectral Density')\n",
    "    plt.title(subjID + ', Motor Imagery Epochs, Pz, Run ' + str(runOfInt))\n",
    "    lgd = plt.legend(['Beta Values Below Threshold','Beta Values Above Threshold','Threshold'], bbox_to_anchor=(1.04,1), loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    fig.savefig('Figures/' + subjID + '_Pz_beta_Session' + str(session_number) + '_R' + str(runOfInt) + '.png',\n",
    "               bbox_extra_artists=(lgd,), bbox_inches='tight')\n",
    "\n",
    "    fig = plt.figure(figsize=(7,6))\n",
    "\n",
    "    # Plot different color for whether the example was actually erroneous or not (true label)\n",
    "    plot_error = [i for i, val in enumerate(TL_all[runOfInt]) if val>0.5]\n",
    "    plot_no_error = [i for i, val in enumerate(TL_all[runOfInt]) if val<0.5]\n",
    "\n",
    "    plt.plot(plot_error, np.asarray(attention_beta_all[runOfInt]['Pz'])[plot_error], 'ro')\n",
    "    plt.plot(plot_no_error, np.asarray(attention_beta_all[runOfInt]['Pz'])[plot_no_error], 'bo')\n",
    "\n",
    "    plt.xlabel('Example')\n",
    "    plt.ylabel('Pz Beta Value')\n",
    "    plt.title(subjID + ', Attention for Erroneous and Non-erroneous movement, Run ' + str(runOfInt))\n",
    "    lgd = plt.legend(['Error Epoch (true label)','Non-error Epoch (true label)'], bbox_to_anchor=(1.04,1), loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    fig.savefig('Figures/' + subjID + '_Pz_beta_color_Session' + str(session_number) + '_R' + str(runOfInt) + '.png',\n",
    "               bbox_extra_artists=(lgd,), bbox_inches='tight')\n",
    "\n",
    "    fig = plt.figure(figsize=(8,6))\n",
    "    plt.plot(CS_all[runOfInt],'ko')\n",
    "    plt.xlabel('Examples')\n",
    "    plt.ylabel('Confidence Score')\n",
    "    plt.title(subjID + ', Confidence Scores, Run ' + str(runOfInt))\n",
    "    plt.show()\n",
    "\n",
    "    fig.savefig('Figures/' + subjID + '_Confidence_Scores_Session' + str(session_number) + '_R' + str(runOfInt) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
